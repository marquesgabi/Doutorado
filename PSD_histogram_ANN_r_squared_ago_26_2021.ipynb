{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_ANN_r_squared_ago_26_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marquesgabi/Doutorado/blob/master/PSD_histogram_ANN_r_squared_ago_26_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117b22d6-f178-4964-fde1-19962122b0bf"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mahotas in /usr/local/lib/python3.7/dist-packages (1.4.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cbdc356-20bd-4a3e-cd1f-d7d77c75f2f1"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'marquesgabi_fev_2020' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0930d9b7-fb4d-404b-cb3a-4ec4ee7e5e8d"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Doutorado' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff4ca0e-6821-4f24-c8af-218f3ae5e66a"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48eeb462-5596-4485-e11f-d91662f64307"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     115  141.695786  147.234390  ...  128.119171  130.903732  132.580414\n",
            "1     128  165.996094  146.831055  ...   81.041016   82.387695   74.822266\n",
            "2     134  172.952103  169.964355  ...  143.671417  144.027191  146.021393\n",
            "3     111  111.947647  109.774773  ...  141.971832  133.793701  129.556854\n",
            "4     151  138.002853  139.104843  ...  108.045616  102.112808  104.191574\n",
            "5     120  193.232208  198.575562  ...  182.655548  130.171112  126.203339\n",
            "6     181  157.589935  169.608002  ...  171.962616  168.681305  162.485641\n",
            "7     147  185.519287  188.031738  ...  152.990936  156.698425  114.589569\n",
            "8     198    0.191511    1.607285  ...  131.915207  133.950607  133.326599\n",
            "9     185  106.681686  117.584015  ...  201.877289  186.108856  190.644440\n",
            "10    146  139.416214  142.092514  ...    1.000000    1.000000    1.000000\n",
            "11    183  181.627106  182.007401  ...  151.705841  151.393585  151.030609\n",
            "12    199  159.931030  161.171036  ...    0.554582    1.606651    0.459105\n",
            "13    168  138.138885  183.916672  ...  186.555557  193.722229  204.277786\n",
            "14    161  223.427216  217.854446  ...  129.264664  127.733459  123.691879\n",
            "15    129  161.583862  162.006683  ...  145.342880  149.447693  151.000000\n",
            "16    197  129.531448  171.207443  ...  180.841553  134.779175  122.795731\n",
            "17    173  102.288712  114.961105  ...  110.267296  110.865814  119.947701\n",
            "18    187  129.297089  130.683914  ...  137.368805  133.333374  122.942978\n",
            "19    113  197.836945  197.824097  ...   36.685253   66.485939  146.241913\n",
            "20    193  158.810013  166.131927  ...  139.451447  109.459976   87.573120\n",
            "21    189  113.565163  104.352539  ...    1.355281    0.155007    1.318244\n",
            "22    138  181.087585  178.133606  ...  146.850021  161.542740  182.844574\n",
            "23    192  186.930984  156.777771  ...  166.201828  159.491302  164.664047\n",
            "24    105  113.515564  117.835571  ...  175.768906  176.866699  176.528885\n",
            "25    152  118.074783   79.621193  ...  120.781151  132.978531  146.976456\n",
            "26    155   78.669060   79.330116  ...    1.000000    1.000000    1.000000\n",
            "27    126  140.246918  142.814819  ...    0.160494    0.604938    1.493827\n",
            "28    138  159.462509  160.066162  ...  126.492538  119.152908  145.704041\n",
            "29    135  142.502762  144.458649  ...   86.890862   90.613487   97.882523\n",
            "30    189  134.629639  150.851852  ...   84.596710  113.621399  120.965698\n",
            "31    112  187.625000  186.500000  ...  194.250000  186.062500  184.000000\n",
            "32    162  236.671234  227.630554  ...    1.428898    0.593812    0.721232\n",
            "33    165  125.562538  118.109467  ...   11.773664    8.933407    3.480551\n",
            "34    197    0.329743    1.611817  ...  117.056252  108.823837   95.722336\n",
            "35    129  166.183411  219.809631  ...  173.427444  169.078308  163.454239\n",
            "36    151  116.425209  112.928078  ...    0.637297    0.345906    1.418754\n",
            "37    168  100.388893  110.250000  ...  129.972229  114.638893  102.722221\n",
            "38    170  135.830460  134.136612  ...  129.616760  140.067566  161.058273\n",
            "39    155  171.894897  162.266785  ...  187.841003  184.710236  189.358963\n",
            "40    119  185.730087  192.892715  ...  208.349472  224.138428  237.723190\n",
            "41    180  185.766937  133.631119  ...  182.333847  152.038040  127.321487\n",
            "42    175  148.555191  142.436798  ...  184.356796  166.084793  179.956802\n",
            "43    159   98.083496  124.177162  ...  150.830185  159.504837  172.505264\n",
            "44    146  122.068871  122.082016  ...   81.026833   95.418457  107.496155\n",
            "45    199  129.393814  147.981277  ...  155.869781  125.521065  152.519165\n",
            "46    112  246.375000  197.437500  ...  124.562500  119.500000  137.375000\n",
            "47    145  131.570602  140.083038  ...  184.144547  171.844971  165.126511\n",
            "48    165   93.604744   94.422440  ...    0.842277    0.216676    1.368191\n",
            "49    182  135.443802   96.556229  ...  174.934921  174.514801  172.940857\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "35036735-6394-4f7e-d834-8792614d566c"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "11125d26-92af-43e3-a529-af7e5890633d"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = 'ANN without convolution '\n",
        "N1 = 200\n",
        "N2 = 10\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "fff06e4d-2783-4940-b8f1-89b7d18f92d2"
      },
      "source": [
        "\n",
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 17ms/step - loss: 0.6918 - accuracy: 0.5073 - val_loss: 0.6892 - val_accuracy: 0.4898\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6850 - accuracy: 0.5044 - val_loss: 0.6844 - val_accuracy: 0.4898\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6830 - accuracy: 0.5277 - val_loss: 0.6787 - val_accuracy: 0.5578\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6797 - accuracy: 0.5364 - val_loss: 0.6729 - val_accuracy: 0.5510\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6701 - accuracy: 0.5889 - val_loss: 0.6655 - val_accuracy: 0.5850\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6006 - val_loss: 0.6587 - val_accuracy: 0.5714\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6539 - accuracy: 0.6385 - val_loss: 0.6486 - val_accuracy: 0.6259\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6436 - accuracy: 0.6472 - val_loss: 0.6392 - val_accuracy: 0.6190\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6322 - accuracy: 0.6676 - val_loss: 0.6253 - val_accuracy: 0.6667\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6227 - accuracy: 0.7055 - val_loss: 0.6123 - val_accuracy: 0.6667\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6140 - accuracy: 0.7434 - val_loss: 0.5945 - val_accuracy: 0.7823\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.5974 - accuracy: 0.7464 - val_loss: 0.5786 - val_accuracy: 0.7687\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.5764 - accuracy: 0.7697 - val_loss: 0.5590 - val_accuracy: 0.8027\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.5641 - accuracy: 0.8017 - val_loss: 0.5399 - val_accuracy: 0.8027\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5446 - accuracy: 0.8192 - val_loss: 0.5190 - val_accuracy: 0.8571\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.5320 - accuracy: 0.8542 - val_loss: 0.5009 - val_accuracy: 0.8163\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.5077 - accuracy: 0.8222 - val_loss: 0.4793 - val_accuracy: 0.8912\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.4913 - accuracy: 0.8630 - val_loss: 0.4598 - val_accuracy: 0.9184\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.4751 - accuracy: 0.8834 - val_loss: 0.4427 - val_accuracy: 0.8980\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.4483 - accuracy: 0.8834 - val_loss: 0.4215 - val_accuracy: 0.9048\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.4546 - accuracy: 0.8630 - val_loss: 0.4035 - val_accuracy: 0.9184\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.4284 - accuracy: 0.8863 - val_loss: 0.3967 - val_accuracy: 0.8980\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.4167 - accuracy: 0.8776 - val_loss: 0.3729 - val_accuracy: 0.9184\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.4102 - accuracy: 0.8834 - val_loss: 0.3571 - val_accuracy: 0.9388\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3812 - accuracy: 0.8950 - val_loss: 0.3721 - val_accuracy: 0.8776\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.4104 - accuracy: 0.8659 - val_loss: 0.3334 - val_accuracy: 0.9524\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3685 - accuracy: 0.9038 - val_loss: 0.3229 - val_accuracy: 0.9320\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3806 - accuracy: 0.9009 - val_loss: 0.3142 - val_accuracy: 0.9320\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3528 - accuracy: 0.8834 - val_loss: 0.3087 - val_accuracy: 0.9320\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3366 - accuracy: 0.9067 - val_loss: 0.2930 - val_accuracy: 0.9456\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3522 - accuracy: 0.8688 - val_loss: 0.2854 - val_accuracy: 0.9456\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3313 - accuracy: 0.8863 - val_loss: 0.2772 - val_accuracy: 0.9524\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.3350 - accuracy: 0.8746 - val_loss: 0.2849 - val_accuracy: 0.9320\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3189 - accuracy: 0.9096 - val_loss: 0.2665 - val_accuracy: 0.9388\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3160 - accuracy: 0.8921 - val_loss: 0.2554 - val_accuracy: 0.9524\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8950 - val_loss: 0.2555 - val_accuracy: 0.9320\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.3084 - accuracy: 0.8980 - val_loss: 0.2558 - val_accuracy: 0.9320\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2972 - accuracy: 0.9009 - val_loss: 0.2408 - val_accuracy: 0.9524\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3011 - accuracy: 0.8921 - val_loss: 0.2523 - val_accuracy: 0.9320\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2989 - accuracy: 0.8921 - val_loss: 0.2319 - val_accuracy: 0.9524\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2776 - accuracy: 0.9213 - val_loss: 0.2411 - val_accuracy: 0.9388\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2760 - accuracy: 0.9155 - val_loss: 0.2335 - val_accuracy: 0.9456\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3046 - accuracy: 0.8805 - val_loss: 0.2290 - val_accuracy: 0.9456\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2719 - accuracy: 0.9096 - val_loss: 0.2181 - val_accuracy: 0.9524\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2893 - accuracy: 0.9067 - val_loss: 0.2230 - val_accuracy: 0.9456\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2809 - accuracy: 0.8980 - val_loss: 0.2116 - val_accuracy: 0.9524\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2751 - accuracy: 0.9155 - val_loss: 0.2102 - val_accuracy: 0.9524\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2527 - accuracy: 0.9038 - val_loss: 0.2097 - val_accuracy: 0.9524\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2447 - accuracy: 0.9184 - val_loss: 0.2075 - val_accuracy: 0.9524\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2398 - accuracy: 0.9184 - val_loss: 0.1976 - val_accuracy: 0.9524\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2817 - accuracy: 0.8921 - val_loss: 0.2284 - val_accuracy: 0.9320\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2672 - accuracy: 0.9155 - val_loss: 0.2003 - val_accuracy: 0.9388\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.2668 - accuracy: 0.8950 - val_loss: 0.1932 - val_accuracy: 0.9524\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2476 - accuracy: 0.9125 - val_loss: 0.1945 - val_accuracy: 0.9592\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2483 - accuracy: 0.9096 - val_loss: 0.1883 - val_accuracy: 0.9592\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2509 - accuracy: 0.9125 - val_loss: 0.1893 - val_accuracy: 0.9592\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2536 - accuracy: 0.9184 - val_loss: 0.1862 - val_accuracy: 0.9592\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2406 - accuracy: 0.9271 - val_loss: 0.1830 - val_accuracy: 0.9660\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2490 - accuracy: 0.9184 - val_loss: 0.1823 - val_accuracy: 0.9524\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2375 - accuracy: 0.9271 - val_loss: 0.1801 - val_accuracy: 0.9660\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2351 - accuracy: 0.9184 - val_loss: 0.1987 - val_accuracy: 0.9388\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2433 - accuracy: 0.9125 - val_loss: 0.1806 - val_accuracy: 0.9524\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2561 - accuracy: 0.8863 - val_loss: 0.1913 - val_accuracy: 0.9252\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2615 - accuracy: 0.9067 - val_loss: 0.2044 - val_accuracy: 0.9456\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2254 - accuracy: 0.9300 - val_loss: 0.1831 - val_accuracy: 0.9388\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2311 - accuracy: 0.9213 - val_loss: 0.1763 - val_accuracy: 0.9524\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2358 - accuracy: 0.9096 - val_loss: 0.1719 - val_accuracy: 0.9592\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2225 - accuracy: 0.9359 - val_loss: 0.1676 - val_accuracy: 0.9660\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2152 - accuracy: 0.9271 - val_loss: 0.1718 - val_accuracy: 0.9388\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2369 - accuracy: 0.9125 - val_loss: 0.1862 - val_accuracy: 0.9456\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2528 - accuracy: 0.9067 - val_loss: 0.1738 - val_accuracy: 0.9388\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2323 - accuracy: 0.9184 - val_loss: 0.1673 - val_accuracy: 0.9524\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2177 - accuracy: 0.9184 - val_loss: 0.1658 - val_accuracy: 0.9592\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2138 - accuracy: 0.9125 - val_loss: 0.1591 - val_accuracy: 0.9660\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2050 - accuracy: 0.9359 - val_loss: 0.1574 - val_accuracy: 0.9660\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2165 - accuracy: 0.9359 - val_loss: 0.1676 - val_accuracy: 0.9524\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1996 - accuracy: 0.9388 - val_loss: 0.1545 - val_accuracy: 0.9660\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2152 - accuracy: 0.9155 - val_loss: 0.1660 - val_accuracy: 0.9524\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2115 - accuracy: 0.9300 - val_loss: 0.1524 - val_accuracy: 0.9660\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2272 - accuracy: 0.9242 - val_loss: 0.1507 - val_accuracy: 0.9660\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1951 - accuracy: 0.9475 - val_loss: 0.1513 - val_accuracy: 0.9660\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2197 - accuracy: 0.9271 - val_loss: 0.1641 - val_accuracy: 0.9456\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2048 - accuracy: 0.9359 - val_loss: 0.1480 - val_accuracy: 0.9660\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2043 - accuracy: 0.9300 - val_loss: 0.1465 - val_accuracy: 0.9660\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1974 - accuracy: 0.9388 - val_loss: 0.1458 - val_accuracy: 0.9660\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1805 - accuracy: 0.9534 - val_loss: 0.1525 - val_accuracy: 0.9592\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1989 - accuracy: 0.9359 - val_loss: 0.1442 - val_accuracy: 0.9660\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1981 - accuracy: 0.9359 - val_loss: 0.1408 - val_accuracy: 0.9660\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2043 - accuracy: 0.9271 - val_loss: 0.1396 - val_accuracy: 0.9660\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2125 - accuracy: 0.9359 - val_loss: 0.1396 - val_accuracy: 0.9660\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1970 - accuracy: 0.9359 - val_loss: 0.1407 - val_accuracy: 0.9660\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9329 - val_loss: 0.1394 - val_accuracy: 0.9660\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1846 - accuracy: 0.9359 - val_loss: 0.1387 - val_accuracy: 0.9660\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1917 - accuracy: 0.9300 - val_loss: 0.1490 - val_accuracy: 0.9660\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2035 - accuracy: 0.9300 - val_loss: 0.1519 - val_accuracy: 0.9320\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1960 - accuracy: 0.9359 - val_loss: 0.1769 - val_accuracy: 0.9456\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1865 - accuracy: 0.9417 - val_loss: 0.1341 - val_accuracy: 0.9660\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1932 - accuracy: 0.9329 - val_loss: 0.1329 - val_accuracy: 0.9660\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1813 - accuracy: 0.9417 - val_loss: 0.1320 - val_accuracy: 0.9592\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1680 - accuracy: 0.9446 - val_loss: 0.1310 - val_accuracy: 0.9592\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1806 - accuracy: 0.9329 - val_loss: 0.1299 - val_accuracy: 0.9592\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1780 - accuracy: 0.9534 - val_loss: 0.1304 - val_accuracy: 0.9592\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1701 - accuracy: 0.9475 - val_loss: 0.1290 - val_accuracy: 0.9660\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1918 - accuracy: 0.9329 - val_loss: 0.1255 - val_accuracy: 0.9592\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1753 - accuracy: 0.9388 - val_loss: 0.1291 - val_accuracy: 0.9660\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1717 - accuracy: 0.9388 - val_loss: 0.1256 - val_accuracy: 0.9592\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1680 - accuracy: 0.9388 - val_loss: 0.1290 - val_accuracy: 0.9592\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1899 - accuracy: 0.9359 - val_loss: 0.1241 - val_accuracy: 0.9592\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1808 - accuracy: 0.9388 - val_loss: 0.1248 - val_accuracy: 0.9592\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1795 - accuracy: 0.9271 - val_loss: 0.1595 - val_accuracy: 0.9592\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1836 - accuracy: 0.9329 - val_loss: 0.1395 - val_accuracy: 0.9388\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1905 - accuracy: 0.9417 - val_loss: 0.1322 - val_accuracy: 0.9660\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1524 - accuracy: 0.9563 - val_loss: 0.1202 - val_accuracy: 0.9592\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1584 - accuracy: 0.9446 - val_loss: 0.1205 - val_accuracy: 0.9592\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1495 - accuracy: 0.9592 - val_loss: 0.1182 - val_accuracy: 0.9592\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9417 - val_loss: 0.1314 - val_accuracy: 0.9524\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1638 - accuracy: 0.9534 - val_loss: 0.1581 - val_accuracy: 0.9524\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1886 - accuracy: 0.9242 - val_loss: 0.1325 - val_accuracy: 0.9456\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1749 - accuracy: 0.9417 - val_loss: 0.1448 - val_accuracy: 0.9660\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1585 - accuracy: 0.9446 - val_loss: 0.1284 - val_accuracy: 0.9592\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1628 - accuracy: 0.9504 - val_loss: 0.1247 - val_accuracy: 0.9660\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1581 - accuracy: 0.9475 - val_loss: 0.1207 - val_accuracy: 0.9592\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1597 - accuracy: 0.9534 - val_loss: 0.1169 - val_accuracy: 0.9524\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1627 - accuracy: 0.9563 - val_loss: 0.1303 - val_accuracy: 0.9728\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1729 - accuracy: 0.9388 - val_loss: 0.1279 - val_accuracy: 0.9456\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1598 - accuracy: 0.9563 - val_loss: 0.1208 - val_accuracy: 0.9592\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1583 - accuracy: 0.9475 - val_loss: 0.1137 - val_accuracy: 0.9592\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1621 - accuracy: 0.9446 - val_loss: 0.1149 - val_accuracy: 0.9660\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1521 - accuracy: 0.9534 - val_loss: 0.1121 - val_accuracy: 0.9592\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1626 - accuracy: 0.9359 - val_loss: 0.1111 - val_accuracy: 0.9592\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1490 - accuracy: 0.9621 - val_loss: 0.1107 - val_accuracy: 0.9660\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1632 - accuracy: 0.9592 - val_loss: 0.1110 - val_accuracy: 0.9660\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1435 - accuracy: 0.9650 - val_loss: 0.1075 - val_accuracy: 0.9592\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1562 - accuracy: 0.9563 - val_loss: 0.1090 - val_accuracy: 0.9660\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1512 - accuracy: 0.9563 - val_loss: 0.1084 - val_accuracy: 0.9660\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1840 - accuracy: 0.9359 - val_loss: 0.1166 - val_accuracy: 0.9660\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1512 - accuracy: 0.9475 - val_loss: 0.1133 - val_accuracy: 0.9592\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1725 - accuracy: 0.9388 - val_loss: 0.1120 - val_accuracy: 0.9592\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1410 - accuracy: 0.9650 - val_loss: 0.1175 - val_accuracy: 0.9660\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1355 - accuracy: 0.9563 - val_loss: 0.1053 - val_accuracy: 0.9592\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1485 - accuracy: 0.9563 - val_loss: 0.1057 - val_accuracy: 0.9660\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1445 - accuracy: 0.9592 - val_loss: 0.1155 - val_accuracy: 0.9660\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1525 - accuracy: 0.9504 - val_loss: 0.1050 - val_accuracy: 0.9660\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1471 - accuracy: 0.9650 - val_loss: 0.1103 - val_accuracy: 0.9728\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1329 - accuracy: 0.9592 - val_loss: 0.1088 - val_accuracy: 0.9592\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1393 - accuracy: 0.9650 - val_loss: 0.1028 - val_accuracy: 0.9592\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1374 - accuracy: 0.9592 - val_loss: 0.1026 - val_accuracy: 0.9592\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.9563 - val_loss: 0.1022 - val_accuracy: 0.9660\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1283 - accuracy: 0.9592 - val_loss: 0.1006 - val_accuracy: 0.9592\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1232 - accuracy: 0.9679 - val_loss: 0.0989 - val_accuracy: 0.9660\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1328 - accuracy: 0.9621 - val_loss: 0.0990 - val_accuracy: 0.9592\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1297 - accuracy: 0.9621 - val_loss: 0.1046 - val_accuracy: 0.9728\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1413 - accuracy: 0.9592 - val_loss: 0.0993 - val_accuracy: 0.9660\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1481 - accuracy: 0.9504 - val_loss: 0.0981 - val_accuracy: 0.9660\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1317 - accuracy: 0.9534 - val_loss: 0.1097 - val_accuracy: 0.9660\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1403 - accuracy: 0.9621 - val_loss: 0.1041 - val_accuracy: 0.9524\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1301 - accuracy: 0.9650 - val_loss: 0.1056 - val_accuracy: 0.9728\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1538 - accuracy: 0.9592 - val_loss: 0.0964 - val_accuracy: 0.9660\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1325 - accuracy: 0.9679 - val_loss: 0.1077 - val_accuracy: 0.9728\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1312 - accuracy: 0.9592 - val_loss: 0.1096 - val_accuracy: 0.9524\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1264 - accuracy: 0.9534 - val_loss: 0.1002 - val_accuracy: 0.9796\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1295 - accuracy: 0.9621 - val_loss: 0.0974 - val_accuracy: 0.9660\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1189 - accuracy: 0.9708 - val_loss: 0.0984 - val_accuracy: 0.9796\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1441 - accuracy: 0.9534 - val_loss: 0.0946 - val_accuracy: 0.9660\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.9592 - val_loss: 0.0966 - val_accuracy: 0.9592\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1252 - accuracy: 0.9534 - val_loss: 0.0989 - val_accuracy: 0.9796\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1324 - accuracy: 0.9621 - val_loss: 0.0938 - val_accuracy: 0.9660\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9504 - val_loss: 0.0947 - val_accuracy: 0.9660\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1543 - accuracy: 0.9475 - val_loss: 0.1118 - val_accuracy: 0.9660\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1303 - accuracy: 0.9650 - val_loss: 0.0940 - val_accuracy: 0.9660\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1314 - accuracy: 0.9534 - val_loss: 0.0961 - val_accuracy: 0.9728\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1389 - accuracy: 0.9563 - val_loss: 0.1035 - val_accuracy: 0.9728\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1289 - accuracy: 0.9329 - val_loss: 0.1067 - val_accuracy: 0.9524\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1199 - accuracy: 0.9534 - val_loss: 0.0953 - val_accuracy: 0.9728\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1442 - accuracy: 0.9388 - val_loss: 0.1138 - val_accuracy: 0.9660\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1364 - accuracy: 0.9475 - val_loss: 0.1111 - val_accuracy: 0.9524\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9621 - val_loss: 0.1188 - val_accuracy: 0.9660\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1231 - accuracy: 0.9592 - val_loss: 0.0942 - val_accuracy: 0.9660\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1217 - accuracy: 0.9650 - val_loss: 0.0932 - val_accuracy: 0.9660\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1115 - accuracy: 0.9650 - val_loss: 0.0968 - val_accuracy: 0.9796\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1313 - accuracy: 0.9592 - val_loss: 0.0971 - val_accuracy: 0.9524\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1160 - accuracy: 0.9738 - val_loss: 0.0965 - val_accuracy: 0.9728\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1061 - accuracy: 0.9679 - val_loss: 0.0935 - val_accuracy: 0.9728\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1201 - accuracy: 0.9708 - val_loss: 0.0920 - val_accuracy: 0.9660\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1003 - accuracy: 0.9708 - val_loss: 0.0923 - val_accuracy: 0.9796\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1125 - accuracy: 0.9592 - val_loss: 0.0977 - val_accuracy: 0.9796\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1202 - accuracy: 0.9679 - val_loss: 0.0927 - val_accuracy: 0.9660\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0964 - accuracy: 0.9679 - val_loss: 0.0977 - val_accuracy: 0.9796\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1056 - accuracy: 0.9650 - val_loss: 0.0970 - val_accuracy: 0.9796\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1222 - accuracy: 0.9592 - val_loss: 0.0921 - val_accuracy: 0.9660\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1146 - accuracy: 0.9708 - val_loss: 0.0960 - val_accuracy: 0.9796\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1336 - accuracy: 0.9563 - val_loss: 0.1050 - val_accuracy: 0.9796\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1201 - accuracy: 0.9563 - val_loss: 0.0938 - val_accuracy: 0.9660\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1000 - accuracy: 0.9650 - val_loss: 0.0943 - val_accuracy: 0.9660\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1146 - accuracy: 0.9650 - val_loss: 0.1050 - val_accuracy: 0.9524\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1229 - accuracy: 0.9621 - val_loss: 0.0982 - val_accuracy: 0.9796\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0985 - accuracy: 0.9825 - val_loss: 0.0965 - val_accuracy: 0.9796\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1083 - accuracy: 0.9738 - val_loss: 0.0949 - val_accuracy: 0.9660\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1094 - accuracy: 0.9621 - val_loss: 0.0957 - val_accuracy: 0.9592\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1069 - accuracy: 0.9650 - val_loss: 0.1193 - val_accuracy: 0.9660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJTKDERlGBIA",
        "outputId": "f0f11df9-fe05-431a-d120-2491c3feb855"
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        72   0\n",
            "1         5  70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "2877a107-d007-48cc-8a72-93076f68b9d5"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNDIL0I5HBzK",
        "outputId": "651b7ff6-e2e2-40ad-a80d-8b947455cae0"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[4] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  #prediction = model.predict_classes(result)\n",
        "  prediction = np.argmax(model.predict(result), axis=-1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0   118.0  107.473999  110.878204  ...  170.419128  176.597519  180.221771\n",
            "1   159.0  106.727379  180.582764  ...   89.819946   98.736832   97.410622\n",
            "2   197.0  160.555069  179.965729  ...  231.494843  170.345413  108.089424\n",
            "3   129.0    0.876630    4.358272  ...  109.857880  137.439392  129.245178\n",
            "5   112.0  189.625000  196.812500  ...  198.875000  210.750000  214.500000\n",
            "6   134.0  148.763885  148.578979  ...  165.326355  156.896423  148.039200\n",
            "10  171.0  231.956604  230.793259  ...  174.045624  172.929474  172.450226\n",
            "11  100.0  162.275208  162.657593  ...  129.329590  126.727997  130.273605\n",
            "12  191.0  153.268158  177.833160  ...  209.557831  217.726624  226.507751\n",
            "13  104.0  126.230789  128.102081  ...  161.930481  162.526642  160.436401\n",
            "14  166.0  178.075180  173.340393  ...   70.684708   88.815056  160.789520\n",
            "16  184.0  162.034012  136.173920  ...  188.140808  200.636078  207.696594\n",
            "17  174.0  153.295959  149.007156  ...    1.991280    0.831682    1.000000\n",
            "18  127.0  193.503647  199.147614  ...  215.370575  214.328094  213.008118\n",
            "19  109.0   78.738823   75.359642  ...  164.304001  167.765244  169.413940\n",
            "21  115.0  171.812469  171.200577  ...  156.013519  154.111145  150.928543\n",
            "22  173.0  174.832642  177.853394  ...   92.151756   33.791473    1.720171\n",
            "23  102.0  139.095749  141.467148  ...  138.851212  144.308350  146.292969\n",
            "24  178.0  168.441238  173.496674  ...  208.041550  209.705963  218.317520\n",
            "26  115.0  173.602036  143.616867  ...  168.082947  168.748123  178.570572\n",
            "30  122.0  179.895447  213.518661  ...  140.082504  128.165268  105.235413\n",
            "32  178.0    0.853680    0.303245  ...  251.400085  254.007111  253.767487\n",
            "33  119.0  180.795837  189.764709  ...  190.965393  181.311417  169.968857\n",
            "36  184.0  112.846397  109.507072  ...    1.284026    0.133270    1.327505\n",
            "38  138.0  108.624657  121.655533  ...  131.084229  131.891418  162.174118\n",
            "39  193.0  145.252869  135.900024  ...    1.000000    1.000000    1.000000\n",
            "40  151.0  120.753693  120.842377  ...  142.990005  193.720734  210.123596\n",
            "41  149.0   82.630692   95.444618  ...  137.202164  148.791229  146.557373\n",
            "42  124.0  161.629547  162.555679  ...  133.141510  138.494263  146.774185\n",
            "43  190.0  195.247192  192.044525  ...    0.997230    0.476233    0.930748\n",
            "44  168.0  180.583328  172.333328  ...  146.722229  113.944443  106.305557\n",
            "45  136.0  134.597763  172.415237  ...  160.335632  163.128891  171.938583\n",
            "46  110.0  179.164948  172.479324  ...  137.177185  162.925949  180.123322\n",
            "48  112.0  162.187500  166.312500  ...  124.937500  151.562500  176.250000\n",
            "49  200.0  130.991989  135.068390  ...  160.928406  198.319183  144.866791\n",
            "\n",
            "[35 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "a8596856-6498-4470-a8b8-fad15f896211"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 687, done.\u001b[K\n",
            "remote: Counting objects: 100% (448/448), done.\u001b[K\n",
            "remote: Compressing objects: 100% (446/446), done.\u001b[K\n",
            "remote: Total 687 (delta 282), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (687/687), 5.59 MiB | 12.40 MiB/s, done.\n",
            "Resolving deltas: 100% (419/419), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "42b7f788-47de-4f29-d73f-56eedb9c4dce"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "%cd marquesgabi_out_2020\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 6.34 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "PekBHQOT_6CP",
        "outputId": "5834e58f-9f6f-402e-c7ec-a475bd2d05bc"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>118.0</td>\n",
              "      <td>107.473999</td>\n",
              "      <td>110.878204</td>\n",
              "      <td>114.124390</td>\n",
              "      <td>123.347603</td>\n",
              "      <td>133.431198</td>\n",
              "      <td>144.207977</td>\n",
              "      <td>142.562195</td>\n",
              "      <td>131.680847</td>\n",
              "      <td>121.959488</td>\n",
              "      <td>115.220627</td>\n",
              "      <td>116.560760</td>\n",
              "      <td>123.886230</td>\n",
              "      <td>122.347031</td>\n",
              "      <td>132.212585</td>\n",
              "      <td>134.191910</td>\n",
              "      <td>130.492386</td>\n",
              "      <td>129.538071</td>\n",
              "      <td>134.886520</td>\n",
              "      <td>136.688568</td>\n",
              "      <td>140.410522</td>\n",
              "      <td>145.409943</td>\n",
              "      <td>150.174667</td>\n",
              "      <td>151.789139</td>\n",
              "      <td>152.482605</td>\n",
              "      <td>153.981323</td>\n",
              "      <td>157.269455</td>\n",
              "      <td>159.634293</td>\n",
              "      <td>158.017517</td>\n",
              "      <td>108.257111</td>\n",
              "      <td>109.849182</td>\n",
              "      <td>110.384094</td>\n",
              "      <td>118.670792</td>\n",
              "      <td>130.389542</td>\n",
              "      <td>135.041656</td>\n",
              "      <td>135.010910</td>\n",
              "      <td>135.827347</td>\n",
              "      <td>136.235855</td>\n",
              "      <td>140.477737</td>\n",
              "      <td>138.734268</td>\n",
              "      <td>...</td>\n",
              "      <td>145.831360</td>\n",
              "      <td>154.898880</td>\n",
              "      <td>168.320877</td>\n",
              "      <td>171.228683</td>\n",
              "      <td>171.605591</td>\n",
              "      <td>167.857788</td>\n",
              "      <td>164.022980</td>\n",
              "      <td>162.028717</td>\n",
              "      <td>162.046234</td>\n",
              "      <td>165.261993</td>\n",
              "      <td>167.972992</td>\n",
              "      <td>169.533463</td>\n",
              "      <td>125.959496</td>\n",
              "      <td>126.444412</td>\n",
              "      <td>127.329498</td>\n",
              "      <td>125.839119</td>\n",
              "      <td>122.983910</td>\n",
              "      <td>124.480034</td>\n",
              "      <td>139.524841</td>\n",
              "      <td>151.489212</td>\n",
              "      <td>152.606140</td>\n",
              "      <td>144.437805</td>\n",
              "      <td>127.866409</td>\n",
              "      <td>108.241028</td>\n",
              "      <td>105.451599</td>\n",
              "      <td>113.951447</td>\n",
              "      <td>124.981613</td>\n",
              "      <td>132.858658</td>\n",
              "      <td>153.094803</td>\n",
              "      <td>163.419418</td>\n",
              "      <td>167.455048</td>\n",
              "      <td>174.018097</td>\n",
              "      <td>179.987076</td>\n",
              "      <td>175.035034</td>\n",
              "      <td>165.583450</td>\n",
              "      <td>162.553864</td>\n",
              "      <td>163.802063</td>\n",
              "      <td>170.419128</td>\n",
              "      <td>176.597519</td>\n",
              "      <td>180.221771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>159.0</td>\n",
              "      <td>106.727379</td>\n",
              "      <td>180.582764</td>\n",
              "      <td>179.350189</td>\n",
              "      <td>143.348602</td>\n",
              "      <td>140.924377</td>\n",
              "      <td>143.439301</td>\n",
              "      <td>130.217377</td>\n",
              "      <td>125.469414</td>\n",
              "      <td>143.330215</td>\n",
              "      <td>147.834442</td>\n",
              "      <td>159.794830</td>\n",
              "      <td>162.990280</td>\n",
              "      <td>160.007950</td>\n",
              "      <td>152.586395</td>\n",
              "      <td>142.025085</td>\n",
              "      <td>140.094696</td>\n",
              "      <td>145.663910</td>\n",
              "      <td>153.929703</td>\n",
              "      <td>154.438080</td>\n",
              "      <td>151.190262</td>\n",
              "      <td>145.809937</td>\n",
              "      <td>142.336334</td>\n",
              "      <td>142.775238</td>\n",
              "      <td>151.528061</td>\n",
              "      <td>170.155640</td>\n",
              "      <td>157.480118</td>\n",
              "      <td>136.541504</td>\n",
              "      <td>135.171143</td>\n",
              "      <td>98.564812</td>\n",
              "      <td>191.676697</td>\n",
              "      <td>184.775589</td>\n",
              "      <td>142.699417</td>\n",
              "      <td>131.200623</td>\n",
              "      <td>143.214081</td>\n",
              "      <td>127.079391</td>\n",
              "      <td>111.949516</td>\n",
              "      <td>142.729279</td>\n",
              "      <td>151.277023</td>\n",
              "      <td>160.317978</td>\n",
              "      <td>...</td>\n",
              "      <td>128.492416</td>\n",
              "      <td>108.525414</td>\n",
              "      <td>94.359283</td>\n",
              "      <td>87.061394</td>\n",
              "      <td>90.016891</td>\n",
              "      <td>84.982124</td>\n",
              "      <td>85.308609</td>\n",
              "      <td>82.845261</td>\n",
              "      <td>80.745773</td>\n",
              "      <td>94.273094</td>\n",
              "      <td>97.775154</td>\n",
              "      <td>99.272888</td>\n",
              "      <td>43.373322</td>\n",
              "      <td>31.463194</td>\n",
              "      <td>30.673153</td>\n",
              "      <td>22.968473</td>\n",
              "      <td>16.371187</td>\n",
              "      <td>17.964874</td>\n",
              "      <td>18.644199</td>\n",
              "      <td>18.193981</td>\n",
              "      <td>31.834383</td>\n",
              "      <td>53.865154</td>\n",
              "      <td>54.869938</td>\n",
              "      <td>51.118664</td>\n",
              "      <td>52.137096</td>\n",
              "      <td>45.918201</td>\n",
              "      <td>67.203392</td>\n",
              "      <td>118.756027</td>\n",
              "      <td>122.243935</td>\n",
              "      <td>118.413788</td>\n",
              "      <td>98.575645</td>\n",
              "      <td>83.923935</td>\n",
              "      <td>88.387680</td>\n",
              "      <td>86.384758</td>\n",
              "      <td>84.776031</td>\n",
              "      <td>85.439453</td>\n",
              "      <td>78.879555</td>\n",
              "      <td>89.819946</td>\n",
              "      <td>98.736832</td>\n",
              "      <td>97.410622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>197.0</td>\n",
              "      <td>160.555069</td>\n",
              "      <td>179.965729</td>\n",
              "      <td>182.872940</td>\n",
              "      <td>183.820419</td>\n",
              "      <td>180.739822</td>\n",
              "      <td>176.425598</td>\n",
              "      <td>174.472229</td>\n",
              "      <td>168.989563</td>\n",
              "      <td>184.518188</td>\n",
              "      <td>196.070496</td>\n",
              "      <td>197.974350</td>\n",
              "      <td>180.190247</td>\n",
              "      <td>176.229630</td>\n",
              "      <td>171.116806</td>\n",
              "      <td>173.500061</td>\n",
              "      <td>174.206512</td>\n",
              "      <td>155.730164</td>\n",
              "      <td>154.649429</td>\n",
              "      <td>157.426147</td>\n",
              "      <td>167.536804</td>\n",
              "      <td>179.032089</td>\n",
              "      <td>188.412445</td>\n",
              "      <td>198.950470</td>\n",
              "      <td>199.116653</td>\n",
              "      <td>179.977020</td>\n",
              "      <td>167.201355</td>\n",
              "      <td>199.837830</td>\n",
              "      <td>186.979675</td>\n",
              "      <td>167.420837</td>\n",
              "      <td>183.051025</td>\n",
              "      <td>191.926163</td>\n",
              "      <td>187.243256</td>\n",
              "      <td>185.095764</td>\n",
              "      <td>184.179932</td>\n",
              "      <td>192.101562</td>\n",
              "      <td>210.108978</td>\n",
              "      <td>227.998169</td>\n",
              "      <td>241.418808</td>\n",
              "      <td>222.245453</td>\n",
              "      <td>...</td>\n",
              "      <td>188.193680</td>\n",
              "      <td>177.811859</td>\n",
              "      <td>200.587189</td>\n",
              "      <td>249.612946</td>\n",
              "      <td>249.823273</td>\n",
              "      <td>247.804886</td>\n",
              "      <td>232.753479</td>\n",
              "      <td>202.939941</td>\n",
              "      <td>210.847137</td>\n",
              "      <td>225.316360</td>\n",
              "      <td>218.073959</td>\n",
              "      <td>176.260468</td>\n",
              "      <td>159.429306</td>\n",
              "      <td>162.371002</td>\n",
              "      <td>176.249542</td>\n",
              "      <td>181.039062</td>\n",
              "      <td>177.243713</td>\n",
              "      <td>152.271576</td>\n",
              "      <td>158.235382</td>\n",
              "      <td>167.084061</td>\n",
              "      <td>194.726624</td>\n",
              "      <td>192.854172</td>\n",
              "      <td>183.845901</td>\n",
              "      <td>182.258926</td>\n",
              "      <td>177.038437</td>\n",
              "      <td>164.311478</td>\n",
              "      <td>158.522827</td>\n",
              "      <td>172.595230</td>\n",
              "      <td>191.656799</td>\n",
              "      <td>198.011917</td>\n",
              "      <td>171.495483</td>\n",
              "      <td>217.324646</td>\n",
              "      <td>251.680786</td>\n",
              "      <td>251.033112</td>\n",
              "      <td>249.568710</td>\n",
              "      <td>244.836990</td>\n",
              "      <td>238.215622</td>\n",
              "      <td>231.494843</td>\n",
              "      <td>170.345413</td>\n",
              "      <td>108.089424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>129.0</td>\n",
              "      <td>0.876630</td>\n",
              "      <td>4.358272</td>\n",
              "      <td>64.782104</td>\n",
              "      <td>88.997955</td>\n",
              "      <td>93.363800</td>\n",
              "      <td>94.284546</td>\n",
              "      <td>69.527313</td>\n",
              "      <td>35.382969</td>\n",
              "      <td>43.043083</td>\n",
              "      <td>39.377323</td>\n",
              "      <td>26.536568</td>\n",
              "      <td>26.651943</td>\n",
              "      <td>48.517033</td>\n",
              "      <td>80.306107</td>\n",
              "      <td>83.818817</td>\n",
              "      <td>98.051620</td>\n",
              "      <td>113.242294</td>\n",
              "      <td>126.937866</td>\n",
              "      <td>123.826324</td>\n",
              "      <td>128.027161</td>\n",
              "      <td>127.718948</td>\n",
              "      <td>124.574181</td>\n",
              "      <td>130.861008</td>\n",
              "      <td>127.814186</td>\n",
              "      <td>125.038467</td>\n",
              "      <td>124.926445</td>\n",
              "      <td>127.590584</td>\n",
              "      <td>127.397568</td>\n",
              "      <td>0.186167</td>\n",
              "      <td>1.547022</td>\n",
              "      <td>53.508865</td>\n",
              "      <td>91.155884</td>\n",
              "      <td>96.204605</td>\n",
              "      <td>99.086777</td>\n",
              "      <td>76.869537</td>\n",
              "      <td>45.070728</td>\n",
              "      <td>44.843456</td>\n",
              "      <td>37.062077</td>\n",
              "      <td>22.684034</td>\n",
              "      <td>...</td>\n",
              "      <td>159.902664</td>\n",
              "      <td>159.652206</td>\n",
              "      <td>156.240189</td>\n",
              "      <td>153.639267</td>\n",
              "      <td>157.255402</td>\n",
              "      <td>158.581406</td>\n",
              "      <td>167.007141</td>\n",
              "      <td>170.482178</td>\n",
              "      <td>158.125275</td>\n",
              "      <td>115.445885</td>\n",
              "      <td>117.799469</td>\n",
              "      <td>120.940147</td>\n",
              "      <td>0.178895</td>\n",
              "      <td>1.093624</td>\n",
              "      <td>1.860405</td>\n",
              "      <td>0.935581</td>\n",
              "      <td>0.028604</td>\n",
              "      <td>0.836128</td>\n",
              "      <td>1.145965</td>\n",
              "      <td>2.991888</td>\n",
              "      <td>32.779221</td>\n",
              "      <td>117.141335</td>\n",
              "      <td>119.878426</td>\n",
              "      <td>115.350464</td>\n",
              "      <td>114.684525</td>\n",
              "      <td>107.554291</td>\n",
              "      <td>93.422386</td>\n",
              "      <td>92.150360</td>\n",
              "      <td>123.296440</td>\n",
              "      <td>145.092712</td>\n",
              "      <td>153.503693</td>\n",
              "      <td>153.031250</td>\n",
              "      <td>156.802658</td>\n",
              "      <td>164.964539</td>\n",
              "      <td>172.138565</td>\n",
              "      <td>170.267761</td>\n",
              "      <td>125.926147</td>\n",
              "      <td>109.857880</td>\n",
              "      <td>137.439392</td>\n",
              "      <td>129.245178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>112.0</td>\n",
              "      <td>189.625000</td>\n",
              "      <td>196.812500</td>\n",
              "      <td>201.437500</td>\n",
              "      <td>202.562500</td>\n",
              "      <td>202.312500</td>\n",
              "      <td>201.312500</td>\n",
              "      <td>199.875000</td>\n",
              "      <td>198.187500</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>195.500000</td>\n",
              "      <td>191.375000</td>\n",
              "      <td>186.812500</td>\n",
              "      <td>180.375000</td>\n",
              "      <td>165.062500</td>\n",
              "      <td>143.750000</td>\n",
              "      <td>124.812500</td>\n",
              "      <td>123.937500</td>\n",
              "      <td>130.250000</td>\n",
              "      <td>130.687500</td>\n",
              "      <td>127.312500</td>\n",
              "      <td>122.187500</td>\n",
              "      <td>126.875000</td>\n",
              "      <td>133.812500</td>\n",
              "      <td>126.875000</td>\n",
              "      <td>126.625000</td>\n",
              "      <td>137.875000</td>\n",
              "      <td>160.375000</td>\n",
              "      <td>188.812500</td>\n",
              "      <td>192.875000</td>\n",
              "      <td>197.687500</td>\n",
              "      <td>201.125000</td>\n",
              "      <td>202.437500</td>\n",
              "      <td>202.562500</td>\n",
              "      <td>202.375000</td>\n",
              "      <td>202.875000</td>\n",
              "      <td>202.375000</td>\n",
              "      <td>202.937500</td>\n",
              "      <td>200.187500</td>\n",
              "      <td>...</td>\n",
              "      <td>110.437500</td>\n",
              "      <td>89.250000</td>\n",
              "      <td>81.750000</td>\n",
              "      <td>77.125000</td>\n",
              "      <td>146.625000</td>\n",
              "      <td>176.125000</td>\n",
              "      <td>180.000000</td>\n",
              "      <td>172.687500</td>\n",
              "      <td>182.812500</td>\n",
              "      <td>198.500000</td>\n",
              "      <td>205.312500</td>\n",
              "      <td>207.187500</td>\n",
              "      <td>170.125000</td>\n",
              "      <td>175.250000</td>\n",
              "      <td>177.812500</td>\n",
              "      <td>172.375000</td>\n",
              "      <td>162.750000</td>\n",
              "      <td>151.000000</td>\n",
              "      <td>143.187500</td>\n",
              "      <td>141.687500</td>\n",
              "      <td>140.562500</td>\n",
              "      <td>155.812500</td>\n",
              "      <td>178.562500</td>\n",
              "      <td>180.250000</td>\n",
              "      <td>163.562500</td>\n",
              "      <td>142.250000</td>\n",
              "      <td>139.250000</td>\n",
              "      <td>135.125000</td>\n",
              "      <td>129.062500</td>\n",
              "      <td>124.812500</td>\n",
              "      <td>104.875000</td>\n",
              "      <td>74.937500</td>\n",
              "      <td>114.562500</td>\n",
              "      <td>171.875000</td>\n",
              "      <td>187.375000</td>\n",
              "      <td>184.687500</td>\n",
              "      <td>190.812500</td>\n",
              "      <td>198.875000</td>\n",
              "      <td>210.750000</td>\n",
              "      <td>214.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width           0           1  ...         781         782         783\n",
              "0  118.0  107.473999  110.878204  ...  170.419128  176.597519  180.221771\n",
              "1  159.0  106.727379  180.582764  ...   89.819946   98.736832   97.410622\n",
              "2  197.0  160.555069  179.965729  ...  231.494843  170.345413  108.089424\n",
              "3  129.0    0.876630    4.358272  ...  109.857880  137.439392  129.245178\n",
              "5  112.0  189.625000  196.812500  ...  198.875000  210.750000  214.500000\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "Area = np.array(PSD_new['Area'])\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J705kDqsE8f",
        "outputId": "8d2a03c4-a9e9-42a7-8750-f4e19f3daa7e"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wCFDX8esLoQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn-F050Hr9Ui"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Vfk_fNXGDK5_",
        "outputId": "6b039339-2736-4a09-b1ac-cd77bb7ecfe7"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f198b89cc50>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nGDbBEeiUij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "88ca2d9a-ee94-40d5-c912-5b600346641d"
      },
      "source": [
        "# plt.hist(x, bins=bins, density=True, histtype='step', cumulative=-1,label='Reversed emp.')\n",
        "plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.07368421, 0.2       , 0.41052632, 0.75789474, 0.88421053,\n",
              "         0.96842105, 0.97894737, 0.97894737, 0.98947368, 1.        ],\n",
              "        [0.11428571, 0.37142857, 0.54285714, 0.65714286, 0.8       ,\n",
              "         0.97142857, 1.        , 1.        , 1.        , 1.        ]]),\n",
              " array([0.75356807, 0.90995518, 1.06634229, 1.22272941, 1.37911652,\n",
              "        1.53550363, 1.69189074, 1.84827785, 2.00466497, 2.16105208,\n",
              "        2.31743919]),\n",
              " <a list of 2 Lists of Patches objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "9xENlBUUxfTu",
        "outputId": "f875df35-9dea-4554-fde4-0a519614b718"
      },
      "source": [
        "Obj = plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')\n",
        "Y1, Y2 = Obj[0]\n",
        "Rsquared = r2_score(Y1, Y2)\n",
        "print('r_squared =',Rsquared)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r_squared = 0.9423652209746187\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KukfpGTTKlj"
      },
      "source": [
        "#df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bT8GFymJAII"
      },
      "source": [
        "# r.history['accuracy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uPdRxL2VwLR"
      },
      "source": [
        "acc_train = r.history['accuracy'][-1]\n",
        "acc_test = r.history['val_accuracy'][-1]\n",
        "loss_train = r.history['loss'][-1]\n",
        "loss_test = r.history['val_loss'][-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTd_-CYN1v0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "14e7f308-5f5d-49bd-fe41-f3f7746b5e6c"
      },
      "source": [
        "\n",
        "df = pd.DataFrame({'N1':N1, 'N2':N2,'R^2':Rsquared,\n",
        "                   'acc train':acc_train,'acc test':acc_test,\n",
        "                   'loss train':loss_train,'loss test':loss_test,\n",
        "                   'Details':Description},\n",
        "                  index= [0])\n",
        "Arq = \"output.xlsx\"\n",
        "df.to_excel(Arq)\n",
        "files.download(Arq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0b01a82e-c6ac-47a3-93f0-b007e519752a\", \"output.xlsx\", 5142)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "ZZHa1j4HT9Dq",
        "outputId": "54d8f55c-ca37-4799-fd79-6bd85b61e099"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<a list of 2 Lists of Patches objects>\n",
            "[0.75356807 0.90995518 1.06634229 1.22272941 1.37911652 1.53550363\n",
            " 1.69189074 1.84827785 2.00466497 2.16105208 2.31743919]\n",
            "[[ 7.36842105 12.63157895 21.05263158 34.73684211 12.63157895  8.42105263\n",
            "   1.05263158  0.          1.05263158  1.05263158]\n",
            " [11.42857143 25.71428571 17.14285714 11.42857143 14.28571429 17.14285714\n",
            "   2.85714286  0.          0.          0.        ]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_vDGeWUwIZ",
        "outputId": "01fcf5e3-3ca8-4c6b-f6fd-fed62b21171e"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200.00000000000014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "KcH52-6iJQ8t",
        "outputId": "67331386-2685-40e0-b598-ee0b0b02a833"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f198fa5d850>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11AxFK_JIii",
        "outputId": "f46fdc87-9bd8-4388-b684-1256c8616fdc"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.59616801403081,\n",
              "  1.0217907939900581,\n",
              "  1.2716187407449044,\n",
              "  1.104429030701514,\n",
              "  1.2163487785097904,\n",
              "  1.6013445735058454,\n",
              "  1.1715597420637607,\n",
              "  1.2534662333717612,\n",
              "  1.2676073151634049,\n",
              "  1.309600575274104,\n",
              "  1.292966945531582,\n",
              "  1.7658322811231006,\n",
              "  1.3564037533648712,\n",
              "  1.2407040781688483,\n",
              "  2.130217298173151,\n",
              "  1.4228319915327,\n",
              "  1.0651086490865755,\n",
              "  1.3008210311003705,\n",
              "  1.336545951796433,\n",
              "  0.8927754224911278,\n",
              "  1.4494292838262302,\n",
              "  1.4052738287907582,\n",
              "  1.6421697097891788,\n",
              "  1.2329833804288621,\n",
              "  1.19042665178928,\n",
              "  1.1682948223612457,\n",
              "  1.1518314137121108,\n",
              "  0.9607802401865855,\n",
              "  2.317439190074449,\n",
              "  1.0591147430338594,\n",
              "  1.4308630919602832,\n",
              "  0.7535680705496237,\n",
              "  0.8608283307581511,\n",
              "  1.2776122636975893,\n",
              "  1.3745862957220916,\n",
              "  1.259546137598783,\n",
              "  1.2978813187979172,\n",
              "  1.2412170838050638,\n",
              "  1.6009469708743893,\n",
              "  1.3149369953539032,\n",
              "  1.417901703622935,\n",
              "  1.2478669653497139,\n",
              "  1.1055812783082735,\n",
              "  0.9561307405997607,\n",
              "  0.9487783503683882,\n",
              "  1.1238565871041026,\n",
              "  1.2058356273089446,\n",
              "  1.2801012827406097,\n",
              "  0.8733100751144249,\n",
              "  0.9194732501297403,\n",
              "  1.6425573339441792,\n",
              "  1.085826790250066,\n",
              "  1.0639125693728595,\n",
              "  1.0875842666474016,\n",
              "  1.417901703622935,\n",
              "  1.550443891425932,\n",
              "  0.7825779328716171,\n",
              "  1.4690612745308145,\n",
              "  1.053086721720641,\n",
              "  1.2676073151634049,\n",
              "  0.7744003006005755,\n",
              "  1.3787482149724068,\n",
              "  1.363892581861956,\n",
              "  1.299352006316543,\n",
              "  1.2870449283923413,\n",
              "  1.11817763925502,\n",
              "  0.9474354220939228,\n",
              "  1.5218484589055707,\n",
              "  1.3526437911676632,\n",
              "  1.1556938532445284,\n",
              "  1.6013445735058454,\n",
              "  1.274619025074578,\n",
              "  1.422384489715834,\n",
              "  1.3408259533459403,\n",
              "  1.172646028567008,\n",
              "  1.1490645795125545,\n",
              "  1.459060149136146,\n",
              "  1.2483770274864237,\n",
              "  1.336545951796433,\n",
              "  0.9601174044814821,\n",
              "  1.4867225193896279,\n",
              "  1.4277452542806772,\n",
              "  1.35028849808504,\n",
              "  0.7560982446653928,\n",
              "  1.259040600296622,\n",
              "  1.13456827900627,\n",
              "  1.6549133695530214,\n",
              "  1.1204526724091788,\n",
              "  1.1176081573544434,\n",
              "  0.9153095762832032,\n",
              "  1.1639273497938836,\n",
              "  1.3066806149514323,\n",
              "  1.1529362882239027,\n",
              "  1.3047303442899274,\n",
              "  1.3066806149514323],\n",
              " [1.0015993095588134,\n",
              "  1.3909276698165756,\n",
              "  1.6676360798140766,\n",
              "  0.9422716806031225,\n",
              "  0.972265000471614,\n",
              "  1.0676397761288767,\n",
              "  1.55561356863368,\n",
              "  0.8384260204079336,\n",
              "  1.640832008303889,\n",
              "  0.9063100376589155,\n",
              "  1.3762793316528734,\n",
              "  1.5639250103907498,\n",
              "  1.3409429925649228,\n",
              "  1.089584127471873,\n",
              "  0.9510996542334005,\n",
              "  0.9827082937307683,\n",
              "  1.3720139005512928,\n",
              "  1.0084704508367224,\n",
              "  1.5129274557040946,\n",
              "  0.9787695672847109,\n",
              "  1.0271211442034325,\n",
              "  1.521061532905015,\n",
              "  0.921580903152168,\n",
              "  1.5869405284614069,\n",
              "  1.1995161410743662,\n",
              "  1.4174519335589326,\n",
              "  1.2834384596141477,\n",
              "  1.2212567105755414,\n",
              "  1.0902549965708415,\n",
              "  1.5774891437368685,\n",
              "  1.4546241502686337,\n",
              "  1.1434136382834446,\n",
              "  0.9042084675361876,\n",
              "  0.8673702617902757,\n",
              "  1.7181486997946478]]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}