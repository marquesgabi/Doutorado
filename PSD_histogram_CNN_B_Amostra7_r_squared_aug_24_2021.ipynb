{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_CNN_B_Amostra7_r_squared_jul_29_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marquesgabi/Doutorado/blob/master/PSD_histogram_CNN_B_Amostra7_r_squared_aug_24_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e378a3-faaa-42f9-ac45-5a685bb8bfd0"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mahotas in /usr/local/lib/python3.7/dist-packages (1.4.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9177e1-becd-4ecb-fb7b-fffd27f5124e"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_fev_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28978cfa-47ca-41e1-b719-7e72964cd787"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip' \n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 405, done.\u001b[K\n",
            "remote: Counting objects: 100% (155/155), done.\u001b[K\n",
            "remote: Compressing objects: 100% (154/154), done.\u001b[K\n",
            "remote: Total 405 (delta 64), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (405/405), 165.54 MiB | 25.12 MiB/s, done.\n",
            "Resolving deltas: 100% (187/187), done.\n",
            "Checking out files: 100% (79/79), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_fev_2020/Doutorado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIYzUcnrdMp",
        "outputId": "ab540b44-233e-41af-b744-1e6dd889ae84"
      },
      "source": [
        "labels =[]\n",
        "with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "  for f in f.namelist():\n",
        "    labels.append(f)\n",
        "print(labels)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Fotos_Grandes-3cdAmostra/Q6-8-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-4.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[2] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb0171b7-6ab0-49b6-d7ab-46b50f1d3a11"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 21.76 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db1411d-5b8b-42ba-8624-99d402964236"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     106  138.077972  148.426147  ...   30.338200   29.640087   27.285513\n",
            "1     121   97.737389  103.739983  ...  142.267761  141.428177  138.138794\n",
            "2     181  104.759506   97.488358  ...   79.088425   93.801842   98.422211\n",
            "3     111   47.960880   50.302895  ...   12.112896   10.170034   10.778751\n",
            "4     127  148.166809  147.467041  ...  149.434509  147.605927  146.045258\n",
            "5     125    0.000000    0.000000  ...   67.136902   69.094467   74.927689\n",
            "6     116  145.191452  149.350769  ...   92.986916   90.632568   87.146255\n",
            "7     193   59.957687   53.407986  ...    7.204488    7.652527    7.097775\n",
            "8     135    0.000000    0.000000  ...  145.081589  149.686203  154.260132\n",
            "9     130   70.875504   66.524734  ...   88.054207   90.479294   91.768753\n",
            "10    155   75.652657   76.183937  ...    8.528824    8.167534    8.027680\n",
            "11    142  101.784973   87.661575  ...  238.538986  246.721100  241.537598\n",
            "12    125    0.000000    0.000000  ...  105.093575  121.628738  139.809357\n",
            "13    113   40.280052   33.543427  ...   50.220688   29.289841    6.534890\n",
            "14    126  157.074097  162.283966  ...   66.629631   65.975311   63.604939\n",
            "15    167   98.418671   99.449966  ...  111.409012  121.676720  124.910477\n",
            "16    145   52.924660   57.984257  ...  140.031479  144.835587  144.760147\n",
            "17    124   52.667011   67.088448  ...  157.935471  180.281982  193.584808\n",
            "18    145  136.989594  122.142120  ...  127.666161  131.336166  128.657745\n",
            "19    171   44.179066   43.835575  ...    7.747615    8.817859    8.654731\n",
            "20    132  110.755745  113.876961  ...    0.516988    0.942149    2.382920\n",
            "21    167   93.697769   95.993088  ...   91.221428   91.051422   84.928650\n",
            "22    127  109.682121  109.281227  ...  198.662079  196.719391  173.647278\n",
            "23    189  104.600822  106.869690  ...    5.469136    5.244170    4.539095\n",
            "24    155  174.875519  182.615784  ...  112.989059  105.980400  102.912689\n",
            "25    124  111.379807  107.964622  ...   59.881371   58.883453   59.138397\n",
            "26    163    0.000000    0.000000  ...  109.527451  110.397339  111.756447\n",
            "27    190  169.734955  151.959213  ...  118.670799  118.486427  114.973526\n",
            "28    119   58.723186   58.086506  ...    8.667821    7.948097    7.519032\n",
            "29    177  147.548477  156.739456  ...  124.497482  124.966118  122.330482\n",
            "30    182   40.757401   56.875744  ...   87.556213   95.875748   99.497055\n",
            "31    192   95.756935   98.192268  ...  102.141487   94.131500   94.312492\n",
            "32    139  157.088715  162.649582  ...  107.522903   99.077217   90.020538\n",
            "33    131  135.835953  142.380676  ...   87.473572   85.230637   81.844467\n",
            "34    151  127.761726  133.906723  ...   85.939262   83.455734   94.599449\n",
            "35    107    0.193205    0.136955  ...   96.899551   90.759628   87.934235\n",
            "36    141  150.947784  147.636597  ...  114.932602  112.234093  108.974899\n",
            "37    163  167.379562  174.111374  ...  198.634399  164.207001   65.153976\n",
            "38    174   94.571548   94.558067  ...   97.936195   97.885193   95.268463\n",
            "39    152  125.626038  131.249298  ...   88.982689   88.825485   91.020081\n",
            "40    153  144.514252  144.504166  ...  211.143982  219.264282  204.768494\n",
            "41    200  143.957611  136.405991  ...    6.814400    6.860400    7.031199\n",
            "42    102  103.472519  105.905426  ...   12.769321   12.520571   13.104192\n",
            "43    124  114.770020  112.958366  ...  124.096771  124.468254  129.167526\n",
            "44    168  123.055557  122.305557  ...  112.277779  112.583336  133.972229\n",
            "45    114  164.422287  161.408447  ...  141.735596  142.767303  139.694992\n",
            "46    152  124.129494  120.033928  ...  120.081024  116.183502  119.639885\n",
            "47    141   69.739502   72.545197  ...   92.801727   88.338661   64.459084\n",
            "48    124  143.487000  145.964600  ...   50.497395   49.360039   50.821014\n",
            "49    185  139.996368  123.329842  ...  139.990326  129.477249  109.858253\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b1e95f90-43b1-474d-9bf9-13a516e49516"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "06315492-cbf5-4c59-f684-51b22d450280"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 21.75 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIFPGE_-vx3T"
      },
      "source": [
        ""
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 32, 64, 128 '\n",
        "N1 = 200\n",
        "N2 = 10\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=32, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "473503e7-9d02-48aa-99d0-3bc85b02c32a"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 2s 87ms/step - loss: 0.6396 - accuracy: 0.7026 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.3547 - accuracy: 0.8426 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.2730 - accuracy: 0.8717 - val_loss: 0.6937 - val_accuracy: 0.4898\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.1406 - accuracy: 0.9475 - val_loss: 0.6931 - val_accuracy: 0.4898\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 0.0987 - accuracy: 0.9621 - val_loss: 0.6940 - val_accuracy: 0.4898\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.1152 - accuracy: 0.9446 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0723 - accuracy: 0.9767 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0429 - accuracy: 0.9825 - val_loss: 0.6937 - val_accuracy: 0.4898\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 0.0331 - accuracy: 0.9796 - val_loss: 0.6959 - val_accuracy: 0.4898\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.6958 - val_accuracy: 0.4898\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0221 - accuracy: 0.9913 - val_loss: 0.7003 - val_accuracy: 0.4898\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0126 - accuracy: 0.9971 - val_loss: 0.6977 - val_accuracy: 0.4898\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0101 - accuracy: 0.9971 - val_loss: 0.7019 - val_accuracy: 0.4898\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0154 - accuracy: 0.9942 - val_loss: 0.7020 - val_accuracy: 0.4898\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0109 - accuracy: 0.9971 - val_loss: 0.7028 - val_accuracy: 0.4898\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.7114 - val_accuracy: 0.4898\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.7152 - val_accuracy: 0.4898\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0072 - accuracy: 0.9971 - val_loss: 0.7138 - val_accuracy: 0.4898\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0049 - accuracy: 0.9971 - val_loss: 0.7095 - val_accuracy: 0.4898\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0638 - accuracy: 0.9854 - val_loss: 0.7108 - val_accuracy: 0.4898\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 0.0251 - accuracy: 0.9883 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0147 - accuracy: 0.9971 - val_loss: 0.6992 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0352 - accuracy: 0.9796 - val_loss: 0.6847 - val_accuracy: 0.6599\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0207 - accuracy: 0.9913 - val_loss: 0.6945 - val_accuracy: 0.4898\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 0.0117 - accuracy: 0.9942 - val_loss: 0.6855 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 0.6859 - val_accuracy: 0.5238\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0062 - accuracy: 0.9971 - val_loss: 0.7090 - val_accuracy: 0.4898\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 0.0187 - accuracy: 0.9913 - val_loss: 0.6739 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0430 - accuracy: 0.9883 - val_loss: 0.7512 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0757 - accuracy: 0.9708 - val_loss: 1.6551 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0898 - accuracy: 0.9796 - val_loss: 2.5580 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.1039 - accuracy: 0.9650 - val_loss: 0.9276 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0385 - accuracy: 0.9854 - val_loss: 1.7443 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0124 - accuracy: 0.9971 - val_loss: 1.7759 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 0.0065 - accuracy: 0.9971 - val_loss: 1.4789 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.4387 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 9.8833e-04 - accuracy: 1.0000 - val_loss: 1.5753 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 66ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.5491 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.4860 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 0.0040 - accuracy: 0.9971 - val_loss: 1.0146 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0042 - accuracy: 0.9971 - val_loss: 1.0927 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.4400 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.7278 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 8.0319e-04 - accuracy: 1.0000 - val_loss: 2.0628 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 8.0937e-04 - accuracy: 1.0000 - val_loss: 2.1018 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 8.7422e-04 - accuracy: 1.0000 - val_loss: 1.8960 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 5.7183e-04 - accuracy: 1.0000 - val_loss: 1.7187 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 7.8412e-04 - accuracy: 1.0000 - val_loss: 1.6403 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 4.3472e-04 - accuracy: 1.0000 - val_loss: 1.7027 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 7.4347e-04 - accuracy: 1.0000 - val_loss: 1.9038 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 6.6623e-04 - accuracy: 1.0000 - val_loss: 2.3501 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 4.0312e-04 - accuracy: 1.0000 - val_loss: 2.6125 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 2.4382e-04 - accuracy: 1.0000 - val_loss: 2.6734 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 2.3792e-04 - accuracy: 1.0000 - val_loss: 2.6625 - val_accuracy: 0.5102\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.3576e-04 - accuracy: 1.0000 - val_loss: 2.6321 - val_accuracy: 0.5102\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 6.1203e-04 - accuracy: 1.0000 - val_loss: 2.5708 - val_accuracy: 0.5102\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 1.5125e-04 - accuracy: 1.0000 - val_loss: 2.5650 - val_accuracy: 0.5102\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 4.7658e-04 - accuracy: 1.0000 - val_loss: 2.1527 - val_accuracy: 0.5170\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 3.3612e-04 - accuracy: 1.0000 - val_loss: 1.6769 - val_accuracy: 0.5510\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 3.3507e-04 - accuracy: 1.0000 - val_loss: 1.6833 - val_accuracy: 0.5850\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.5184 - val_accuracy: 0.6054\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 7.1538e-04 - accuracy: 1.0000 - val_loss: 1.3621 - val_accuracy: 0.6190\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 7.1288e-05 - accuracy: 1.0000 - val_loss: 1.5080 - val_accuracy: 0.6122\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 9.6280e-05 - accuracy: 1.0000 - val_loss: 1.5811 - val_accuracy: 0.6122\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 1.1626e-04 - accuracy: 1.0000 - val_loss: 1.4132 - val_accuracy: 0.6395\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 4.9128e-04 - accuracy: 1.0000 - val_loss: 1.5682 - val_accuracy: 0.6122\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.5452e-04 - accuracy: 1.0000 - val_loss: 1.9395 - val_accuracy: 0.6122\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 1s 65ms/step - loss: 1.9504e-04 - accuracy: 1.0000 - val_loss: 1.7473 - val_accuracy: 0.6327\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 9.0351e-05 - accuracy: 1.0000 - val_loss: 1.1857 - val_accuracy: 0.6939\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 4.8577e-05 - accuracy: 1.0000 - val_loss: 0.9272 - val_accuracy: 0.7483\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 1.1203e-04 - accuracy: 1.0000 - val_loss: 0.5678 - val_accuracy: 0.8435\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 2.8538e-04 - accuracy: 1.0000 - val_loss: 0.4384 - val_accuracy: 0.8844\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 5.4824e-05 - accuracy: 1.0000 - val_loss: 0.4172 - val_accuracy: 0.8912\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 5.5157e-05 - accuracy: 1.0000 - val_loss: 0.3496 - val_accuracy: 0.9116\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 7.4298e-05 - accuracy: 1.0000 - val_loss: 0.2992 - val_accuracy: 0.9184\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 1.4949e-04 - accuracy: 1.0000 - val_loss: 0.2556 - val_accuracy: 0.9184\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 8.9597e-05 - accuracy: 1.0000 - val_loss: 0.2182 - val_accuracy: 0.9456\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 8.4215e-05 - accuracy: 1.0000 - val_loss: 0.1985 - val_accuracy: 0.9456\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 1.1314e-04 - accuracy: 1.0000 - val_loss: 0.1792 - val_accuracy: 0.9524\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 9.9737e-05 - accuracy: 1.0000 - val_loss: 0.1725 - val_accuracy: 0.9524\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 7.8163e-05 - accuracy: 1.0000 - val_loss: 0.1644 - val_accuracy: 0.9524\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 4.5115e-05 - accuracy: 1.0000 - val_loss: 0.1616 - val_accuracy: 0.9524\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.1848e-04 - accuracy: 1.0000 - val_loss: 0.2208 - val_accuracy: 0.9456\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 6.8914e-05 - accuracy: 1.0000 - val_loss: 0.2502 - val_accuracy: 0.9456\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 7.5787e-05 - accuracy: 1.0000 - val_loss: 0.2871 - val_accuracy: 0.9320\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 2.3345e-04 - accuracy: 1.0000 - val_loss: 0.2738 - val_accuracy: 0.9388\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 1.0030e-04 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9252\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 1.0134e-04 - accuracy: 1.0000 - val_loss: 0.2778 - val_accuracy: 0.9252\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 8.2774e-05 - accuracy: 1.0000 - val_loss: 0.2826 - val_accuracy: 0.9252\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.1921 - val_accuracy: 0.6803\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 2.4190e-04 - accuracy: 1.0000 - val_loss: 2.4273 - val_accuracy: 0.5578\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 2.0339e-04 - accuracy: 1.0000 - val_loss: 1.4264 - val_accuracy: 0.6259\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 1.6170e-04 - accuracy: 1.0000 - val_loss: 0.2778 - val_accuracy: 0.9252\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 3.0458e-04 - accuracy: 1.0000 - val_loss: 0.9924 - val_accuracy: 0.7551\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.6253e-04 - accuracy: 1.0000 - val_loss: 3.6069 - val_accuracy: 0.5986\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 3.1260e-04 - accuracy: 1.0000 - val_loss: 4.6396 - val_accuracy: 0.5850\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 1.9006e-04 - accuracy: 1.0000 - val_loss: 4.1811 - val_accuracy: 0.5918\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 3.2410e-05 - accuracy: 1.0000 - val_loss: 2.1001 - val_accuracy: 0.6463\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.0435e-04 - accuracy: 1.0000 - val_loss: 2.8670 - val_accuracy: 0.6122\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 5.1052e-04 - accuracy: 1.0000 - val_loss: 1.0437 - val_accuracy: 0.7347\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 4.7073e-05 - accuracy: 1.0000 - val_loss: 1.3919 - val_accuracy: 0.7007\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 3.2357e-05 - accuracy: 1.0000 - val_loss: 1.3295 - val_accuracy: 0.7007\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 1.5442e-05 - accuracy: 1.0000 - val_loss: 1.2201 - val_accuracy: 0.7211\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 3.8852e-05 - accuracy: 1.0000 - val_loss: 1.1346 - val_accuracy: 0.7143\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 2.9121e-05 - accuracy: 1.0000 - val_loss: 1.0228 - val_accuracy: 0.7347\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 5.1073e-05 - accuracy: 1.0000 - val_loss: 0.8911 - val_accuracy: 0.7619\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 6.1553e-05 - accuracy: 1.0000 - val_loss: 0.7527 - val_accuracy: 0.7755\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 9.9261e-05 - accuracy: 1.0000 - val_loss: 1.1294 - val_accuracy: 0.7415\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.5876e-04 - accuracy: 1.0000 - val_loss: 1.9257 - val_accuracy: 0.6871\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.9334e-05 - accuracy: 1.0000 - val_loss: 4.7987 - val_accuracy: 0.5714\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 5.1211e-05 - accuracy: 1.0000 - val_loss: 5.1927 - val_accuracy: 0.5510\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 2.6726e-04 - accuracy: 1.0000 - val_loss: 6.2577 - val_accuracy: 0.5306\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 4.4573e-05 - accuracy: 1.0000 - val_loss: 4.9226 - val_accuracy: 0.5646\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 3.7456e-05 - accuracy: 1.0000 - val_loss: 4.2276 - val_accuracy: 0.5782\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 1.6791e-05 - accuracy: 1.0000 - val_loss: 3.6812 - val_accuracy: 0.6054\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 7.3991e-05 - accuracy: 1.0000 - val_loss: 2.3864 - val_accuracy: 0.6395\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 5.4261e-05 - accuracy: 1.0000 - val_loss: 1.5070 - val_accuracy: 0.7143\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 3.8569e-05 - accuracy: 1.0000 - val_loss: 1.0299 - val_accuracy: 0.7755\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 2.1075e-04 - accuracy: 1.0000 - val_loss: 0.6874 - val_accuracy: 0.8571\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 2.7267e-05 - accuracy: 1.0000 - val_loss: 0.4603 - val_accuracy: 0.9184\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 8.2127e-05 - accuracy: 1.0000 - val_loss: 0.3610 - val_accuracy: 0.9388\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 8.9009e-05 - accuracy: 1.0000 - val_loss: 0.4017 - val_accuracy: 0.9320\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 2.5329e-05 - accuracy: 1.0000 - val_loss: 0.6180 - val_accuracy: 0.8980\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 3.0558e-05 - accuracy: 1.0000 - val_loss: 0.5931 - val_accuracy: 0.8912\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 1.2997e-04 - accuracy: 1.0000 - val_loss: 0.3700 - val_accuracy: 0.9388\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 2.1836e-05 - accuracy: 1.0000 - val_loss: 0.2363 - val_accuracy: 0.9456\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.6987e-05 - accuracy: 1.0000 - val_loss: 0.2129 - val_accuracy: 0.9592\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 3.4401e-05 - accuracy: 1.0000 - val_loss: 0.1990 - val_accuracy: 0.9592\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 3.0141e-04 - accuracy: 1.0000 - val_loss: 0.1774 - val_accuracy: 0.9456\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 2.0101e-05 - accuracy: 1.0000 - val_loss: 0.1402 - val_accuracy: 0.9728\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 3.6859e-05 - accuracy: 1.0000 - val_loss: 0.1325 - val_accuracy: 0.9728\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 5.4181e-05 - accuracy: 1.0000 - val_loss: 0.1436 - val_accuracy: 0.9660\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 1.7130e-05 - accuracy: 1.0000 - val_loss: 0.1935 - val_accuracy: 0.9524\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 8.0952e-05 - accuracy: 1.0000 - val_loss: 0.3628 - val_accuracy: 0.9320\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 7.3885e-05 - accuracy: 1.0000 - val_loss: 0.4111 - val_accuracy: 0.9320\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 1.5095e-05 - accuracy: 1.0000 - val_loss: 0.4158 - val_accuracy: 0.9320\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 3.2387e-05 - accuracy: 1.0000 - val_loss: 0.3856 - val_accuracy: 0.9252\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 8.8565e-05 - accuracy: 1.0000 - val_loss: 0.7421 - val_accuracy: 0.8844\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 1.8618e-04 - accuracy: 1.0000 - val_loss: 12.9284 - val_accuracy: 0.5102\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 2.4743e-05 - accuracy: 1.0000 - val_loss: 19.3542 - val_accuracy: 0.5102\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 1.4354e-05 - accuracy: 1.0000 - val_loss: 20.0586 - val_accuracy: 0.5102\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 5.1812e-05 - accuracy: 1.0000 - val_loss: 19.1298 - val_accuracy: 0.5102\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 1.5259e-05 - accuracy: 1.0000 - val_loss: 17.6904 - val_accuracy: 0.5102\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 2.6968e-05 - accuracy: 1.0000 - val_loss: 16.2286 - val_accuracy: 0.5102\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 1.5126e-05 - accuracy: 1.0000 - val_loss: 14.8515 - val_accuracy: 0.5102\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 1.5490e-05 - accuracy: 1.0000 - val_loss: 13.5565 - val_accuracy: 0.5102\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 1.4729e-05 - accuracy: 1.0000 - val_loss: 12.3742 - val_accuracy: 0.5102\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 1s 60ms/step - loss: 6.0273e-05 - accuracy: 1.0000 - val_loss: 11.2772 - val_accuracy: 0.5102\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 2.7273e-05 - accuracy: 1.0000 - val_loss: 10.4375 - val_accuracy: 0.5102\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 3.8144e-05 - accuracy: 1.0000 - val_loss: 9.8402 - val_accuracy: 0.5102\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 1.5620e-05 - accuracy: 1.0000 - val_loss: 9.1408 - val_accuracy: 0.5102\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 1.7137e-05 - accuracy: 1.0000 - val_loss: 8.3508 - val_accuracy: 0.5102\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 2.3098e-05 - accuracy: 1.0000 - val_loss: 7.5459 - val_accuracy: 0.5170\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 1.3950e-05 - accuracy: 1.0000 - val_loss: 6.6545 - val_accuracy: 0.5170\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 1s 65ms/step - loss: 1.8950e-05 - accuracy: 1.0000 - val_loss: 5.2773 - val_accuracy: 0.5510\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 2.0368e-05 - accuracy: 1.0000 - val_loss: 4.0576 - val_accuracy: 0.6054\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 1.5494e-05 - accuracy: 1.0000 - val_loss: 3.2486 - val_accuracy: 0.6599\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 1.4140e-05 - accuracy: 1.0000 - val_loss: 2.5740 - val_accuracy: 0.7143\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 7.5509e-06 - accuracy: 1.0000 - val_loss: 2.2017 - val_accuracy: 0.7211\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 2.1333e-05 - accuracy: 1.0000 - val_loss: 1.5312 - val_accuracy: 0.7755\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 1.9633e-05 - accuracy: 1.0000 - val_loss: 1.0512 - val_accuracy: 0.8231\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 1s 66ms/step - loss: 2.4708e-05 - accuracy: 1.0000 - val_loss: 0.8095 - val_accuracy: 0.8435\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 1s 65ms/step - loss: 9.9981e-06 - accuracy: 1.0000 - val_loss: 0.6002 - val_accuracy: 0.8912\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 1.3569e-05 - accuracy: 1.0000 - val_loss: 0.4624 - val_accuracy: 0.9184\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 6.1249e-05 - accuracy: 1.0000 - val_loss: 0.4150 - val_accuracy: 0.9252\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 2.6862e-05 - accuracy: 1.0000 - val_loss: 0.3399 - val_accuracy: 0.9252\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 2.0057e-05 - accuracy: 1.0000 - val_loss: 0.3364 - val_accuracy: 0.9320\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 2.4319e-05 - accuracy: 1.0000 - val_loss: 0.3693 - val_accuracy: 0.9252\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 8.6495e-06 - accuracy: 1.0000 - val_loss: 0.3447 - val_accuracy: 0.9320\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 3.5811e-05 - accuracy: 1.0000 - val_loss: 0.4858 - val_accuracy: 0.9116\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 8.4203e-06 - accuracy: 1.0000 - val_loss: 0.5504 - val_accuracy: 0.9116\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 3.4875e-05 - accuracy: 1.0000 - val_loss: 0.4788 - val_accuracy: 0.9184\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 2.0089e-05 - accuracy: 1.0000 - val_loss: 0.4097 - val_accuracy: 0.9184\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 4.3615e-04 - accuracy: 1.0000 - val_loss: 5.3488 - val_accuracy: 0.5442\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 6.9867e-05 - accuracy: 1.0000 - val_loss: 8.1249 - val_accuracy: 0.5102\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 5.2232e-05 - accuracy: 1.0000 - val_loss: 8.6653 - val_accuracy: 0.5102\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 1s 65ms/step - loss: 8.0159e-05 - accuracy: 1.0000 - val_loss: 8.0432 - val_accuracy: 0.5102\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 4.8206e-05 - accuracy: 1.0000 - val_loss: 8.1357 - val_accuracy: 0.5102\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 9.7661e-06 - accuracy: 1.0000 - val_loss: 7.6705 - val_accuracy: 0.5102\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 1s 66ms/step - loss: 1.5871e-05 - accuracy: 1.0000 - val_loss: 7.0086 - val_accuracy: 0.5102\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 7.0176e-05 - accuracy: 1.0000 - val_loss: 5.7621 - val_accuracy: 0.5306\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 1.0390e-05 - accuracy: 1.0000 - val_loss: 4.6780 - val_accuracy: 0.5578\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 3.3598e-04 - accuracy: 1.0000 - val_loss: 8.5488 - val_accuracy: 0.5102\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 1.9568e-05 - accuracy: 1.0000 - val_loss: 19.4633 - val_accuracy: 0.5102\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 2.9291e-05 - accuracy: 1.0000 - val_loss: 21.6794 - val_accuracy: 0.5102\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 1s 61ms/step - loss: 2.1563e-05 - accuracy: 1.0000 - val_loss: 21.2562 - val_accuracy: 0.5102\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 1s 65ms/step - loss: 2.0154e-05 - accuracy: 1.0000 - val_loss: 19.5326 - val_accuracy: 0.5102\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 1s 65ms/step - loss: 2.9564e-04 - accuracy: 1.0000 - val_loss: 20.9263 - val_accuracy: 0.5102\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 8.6171e-05 - accuracy: 1.0000 - val_loss: 26.6428 - val_accuracy: 0.5102\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 6.4211e-05 - accuracy: 1.0000 - val_loss: 26.2881 - val_accuracy: 0.5102\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 0.0048 - accuracy: 0.9971 - val_loss: 27.3845 - val_accuracy: 0.5102\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 1s 65ms/step - loss: 0.0705 - accuracy: 0.9883 - val_loss: 124.1945 - val_accuracy: 0.5102\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 0.3168 - accuracy: 0.9388 - val_loss: 527.5380 - val_accuracy: 0.5102\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 0.1354 - accuracy: 0.9475 - val_loss: 681.5270 - val_accuracy: 0.5102\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 0.0592 - accuracy: 0.9796 - val_loss: 1398.2849 - val_accuracy: 0.5102\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 0.0777 - accuracy: 0.9825 - val_loss: 1062.6364 - val_accuracy: 0.5102\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 0.0723 - accuracy: 0.9883 - val_loss: 292.1244 - val_accuracy: 0.5102\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 0.0754 - accuracy: 0.9738 - val_loss: 28.0086 - val_accuracy: 0.5102\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 1s 66ms/step - loss: 0.0390 - accuracy: 0.9913 - val_loss: 75.1700 - val_accuracy: 0.4898\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 0.0134 - accuracy: 0.9942 - val_loss: 184.5252 - val_accuracy: 0.4898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlTCuKF-Alnh"
      },
      "source": [
        "#pred_test[0,0]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udwjTMHPA83h"
      },
      "source": [
        ""
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVeiPQLiA86E"
      },
      "source": [
        "#pred_test"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpf0XlSARX78",
        "outputId": "28ce4b11-44ac-44b6-b9bf-feae3809311d"
      },
      "source": [
        "pred_test= model.predict(X_test)\n",
        "Rows, Cols = pred_test.shape\n",
        "Prediction =[]\n",
        "for i in range(Rows):\n",
        "  if(pred_test[0,0] > pred_test[0,1]):\n",
        "    Prediction.append(0)\n",
        "  else:\n",
        "    Prediction.append(1)\n",
        "  \n",
        "\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': Prediction}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict   0\n",
            "Actual     \n",
            "0        72\n",
            "1        75\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iFNNrlWV9tH"
      },
      "source": [
        "#pred_test"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QISvYcJBgWbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828864aa-e492-4569-f284-c627a2460e36"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[2] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  prediction_02 = model.predict(result)\n",
        "  Rows, Cols = prediction_02.shape\n",
        "  Prediction =[]\n",
        "  for i in range(Rows):\n",
        "    if(prediction_02[0,0] > prediction_02[0,1]):\n",
        "      Prediction.append(0)\n",
        "    else:\n",
        "      Prediction.append(1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in Prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0   135.0   97.457939  114.915054  ...  155.639374  165.120438  172.667755\n",
            "1   146.0   87.688873   93.693756  ...  156.610977  157.085190  158.506454\n",
            "2   164.0  149.836411  148.491974  ...  144.801315  145.669266  147.945282\n",
            "3   134.0  109.896652  118.946762  ...   61.026287   61.673206   56.842281\n",
            "4   168.0  146.361115  156.166672  ...  151.388885  157.833328  163.388885\n",
            "5   192.0   87.293396   93.087669  ...    6.079861    6.037326    5.554253\n",
            "6   175.0  103.313606  125.371201  ...    7.790401    8.391999    7.892800\n",
            "7   158.0   60.030441   58.946648  ...    9.317897    9.236660    8.836084\n",
            "8   121.0   88.588898   66.468407  ...   89.830750   97.411720  126.287079\n",
            "9   106.0  115.763977  114.792816  ...  112.762909  113.473473  113.716278\n",
            "10  134.0  185.385834  179.891953  ...   64.568283   57.203613   29.936956\n",
            "11  173.0  119.660591  115.919609  ...  138.053085  134.718536  131.468506\n",
            "12  116.0  121.041618  120.832329  ...   64.637337   60.778831   56.903683\n",
            "13  123.0  122.917328  122.459648  ...   10.488796   11.132858   11.471544\n",
            "14  171.0  154.331833  153.570496  ...  126.984573  127.251839  129.324951\n",
            "15  119.0  133.013840  122.702415  ...   80.252594   83.615913   84.958473\n",
            "16  192.0    2.218750    3.092882  ...   21.718313    2.187500    0.292101\n",
            "17  131.0  138.590454  138.190842  ...  108.790451  109.977325  113.458824\n",
            "18  195.0  121.894737  124.385147  ...   11.751295   26.425934   44.149429\n",
            "19  102.0  161.134964  154.625168  ...  150.104187  155.834686  156.821243\n",
            "20  135.0  168.194229  175.076309  ...   79.630936   81.179375   83.397751\n",
            "21  145.0  152.199844  147.167374  ...  123.416122  117.311859  106.516151\n",
            "22  147.0  160.791382  164.668945  ...  196.607727  202.074829  209.281189\n",
            "23  132.0  135.022964  131.909103  ...  170.851257  170.631775  168.322296\n",
            "24  180.0  147.776810  166.327911  ...   84.351608   83.335312   81.723465\n",
            "25  184.0   85.025040   96.866249  ...    3.958412    1.239603    0.119093\n",
            "26  180.0  106.207420  101.887917  ...   77.165436   77.783714   79.421242\n",
            "27  114.0   94.095421   96.570030  ...   10.703909   11.397969   11.621422\n",
            "28  143.0  108.161568  108.371460  ...  180.378693  154.076279  145.725662\n",
            "29  120.0  114.303329  117.843330  ...  159.747772  156.727783  158.517776\n",
            "30  196.0  150.081635  161.938766  ...    6.571428    7.122449    6.836735\n",
            "31  174.0  114.758095  145.564301  ...   64.458191    4.421853    0.600476\n",
            "32  122.0  146.779892  145.049179  ...  129.828262  127.309853  129.872070\n",
            "33  175.0  114.315201  114.868790  ...   98.059196  109.388802  112.390396\n",
            "34  102.0  124.574791  126.519424  ...  111.546715  112.061905  114.047295\n",
            "35  160.0  144.568741  128.873749  ...   58.379372   57.386879   65.921249\n",
            "36  144.0   81.496918   81.186729  ...   54.559418   58.730713   60.196762\n",
            "37  186.0    0.000000    0.000000  ...  145.675110  148.423859  153.660553\n",
            "38  195.0    4.137094   15.876373  ...    7.496805    7.220777    7.443840\n",
            "39  133.0    0.000000    0.000000  ...  156.418289  172.119110  178.889206\n",
            "40  163.0   73.467003   78.082764  ...    8.740601    9.167413    7.726787\n",
            "41  106.0   49.754715   43.875759  ...   93.987190   95.466362  102.485229\n",
            "42  192.0  139.222641  138.861969  ...  125.927078  128.113708  132.039490\n",
            "43  195.0  134.139679  130.002151  ...  195.161133  206.629303  211.348160\n",
            "44  111.0   94.642975   94.127754  ...   97.723076   99.229935   94.963562\n",
            "45  192.0  140.151031  129.592865  ...  159.174454  157.103729  156.058151\n",
            "46  140.0  145.479996  151.279999  ...  102.919998   96.159996   79.320000\n",
            "47  172.0  114.406700  113.531113  ...  180.662521  154.844254  182.358032\n",
            "48  112.0  101.562500  103.812500  ...   12.437500   11.687500   10.937500\n",
            "49  170.0   97.924988  101.350029  ...    1.113633    0.352941    0.000000\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "79efa014-d6a7-4702-dc50-48cddf841bd7"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 671, done.\u001b[K\n",
            "remote: Counting objects: 100% (432/432), done.\u001b[K\n",
            "remote: Compressing objects: 100% (430/430), done.\u001b[K\n",
            "remote: Total 671 (delta 270), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (671/671), 5.50 MiB | 13.77 MiB/s, done.\n",
            "Resolving deltas: 100% (407/407), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "2d539925-3de2-4ed7-d919-b9e15826906a"
      },
      "source": [
        "#!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "\n",
        "%cd Doutorado\n",
        "\n",
        "PSD_imageJ = 'Amostra7.csv' \n",
        "PSD_new = pd.read_csv(PSD_imageJ,sep=';')\n",
        "#encoding='utf8'\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 405, done.\u001b[K\n",
            "remote: Counting objects: 100% (155/155), done.\u001b[K\n",
            "remote: Compressing objects: 100% (154/154), done.\u001b[K\n",
            "remote: Total 405 (delta 64), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (405/405), 165.54 MiB | 24.40 MiB/s, done.\n",
            "Resolving deltas: 100% (187/187), done.\n",
            "Checking out files: 100% (79/79), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/Doutorado\n",
            "   Unnamed: 0   Area\n",
            "0           1  1.387\n",
            "1           2  1.626\n",
            "2           3  1.336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tEPjIBnv_xM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29921f7-6736-4659-c76e-b29570a2ccc7"
      },
      "source": [
        "PSD_new.shape"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfagXc-Mv3oa"
      },
      "source": [
        ""
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PekBHQOT_6CP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "1c67469c-d9be-4a3a-bfe2-b6872916eb26"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135.0</td>\n",
              "      <td>97.457939</td>\n",
              "      <td>114.915054</td>\n",
              "      <td>140.595001</td>\n",
              "      <td>148.042023</td>\n",
              "      <td>147.830673</td>\n",
              "      <td>146.135193</td>\n",
              "      <td>145.768387</td>\n",
              "      <td>149.826447</td>\n",
              "      <td>155.885864</td>\n",
              "      <td>161.172989</td>\n",
              "      <td>159.810577</td>\n",
              "      <td>151.658585</td>\n",
              "      <td>95.612892</td>\n",
              "      <td>34.860741</td>\n",
              "      <td>55.824356</td>\n",
              "      <td>60.240051</td>\n",
              "      <td>126.681252</td>\n",
              "      <td>137.718399</td>\n",
              "      <td>138.531906</td>\n",
              "      <td>139.598801</td>\n",
              "      <td>140.116531</td>\n",
              "      <td>139.215027</td>\n",
              "      <td>138.623810</td>\n",
              "      <td>133.515427</td>\n",
              "      <td>131.008545</td>\n",
              "      <td>129.416901</td>\n",
              "      <td>126.632149</td>\n",
              "      <td>124.617119</td>\n",
              "      <td>116.571472</td>\n",
              "      <td>128.471161</td>\n",
              "      <td>139.132721</td>\n",
              "      <td>143.025955</td>\n",
              "      <td>144.311432</td>\n",
              "      <td>144.963730</td>\n",
              "      <td>145.767517</td>\n",
              "      <td>147.517517</td>\n",
              "      <td>155.103958</td>\n",
              "      <td>166.355209</td>\n",
              "      <td>172.240875</td>\n",
              "      <td>...</td>\n",
              "      <td>75.956154</td>\n",
              "      <td>76.401154</td>\n",
              "      <td>88.767395</td>\n",
              "      <td>107.180786</td>\n",
              "      <td>119.209000</td>\n",
              "      <td>127.208664</td>\n",
              "      <td>142.952423</td>\n",
              "      <td>151.844727</td>\n",
              "      <td>156.633194</td>\n",
              "      <td>157.332550</td>\n",
              "      <td>163.342422</td>\n",
              "      <td>169.098373</td>\n",
              "      <td>116.921318</td>\n",
              "      <td>119.894043</td>\n",
              "      <td>121.111115</td>\n",
              "      <td>118.710670</td>\n",
              "      <td>117.811409</td>\n",
              "      <td>115.516426</td>\n",
              "      <td>115.794937</td>\n",
              "      <td>116.394226</td>\n",
              "      <td>118.879715</td>\n",
              "      <td>122.538864</td>\n",
              "      <td>124.265610</td>\n",
              "      <td>123.517853</td>\n",
              "      <td>127.594116</td>\n",
              "      <td>123.838844</td>\n",
              "      <td>111.391647</td>\n",
              "      <td>100.026169</td>\n",
              "      <td>92.037033</td>\n",
              "      <td>89.474937</td>\n",
              "      <td>96.222656</td>\n",
              "      <td>107.310280</td>\n",
              "      <td>118.240097</td>\n",
              "      <td>122.912697</td>\n",
              "      <td>130.197205</td>\n",
              "      <td>140.813263</td>\n",
              "      <td>148.545517</td>\n",
              "      <td>155.639374</td>\n",
              "      <td>165.120438</td>\n",
              "      <td>172.667755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>146.0</td>\n",
              "      <td>87.688873</td>\n",
              "      <td>93.693756</td>\n",
              "      <td>100.420715</td>\n",
              "      <td>118.062660</td>\n",
              "      <td>148.431412</td>\n",
              "      <td>156.403641</td>\n",
              "      <td>161.206421</td>\n",
              "      <td>162.693741</td>\n",
              "      <td>158.397827</td>\n",
              "      <td>146.346771</td>\n",
              "      <td>139.046356</td>\n",
              "      <td>137.226120</td>\n",
              "      <td>131.480392</td>\n",
              "      <td>125.829803</td>\n",
              "      <td>124.525620</td>\n",
              "      <td>123.767494</td>\n",
              "      <td>124.028343</td>\n",
              "      <td>120.176590</td>\n",
              "      <td>116.483757</td>\n",
              "      <td>98.240952</td>\n",
              "      <td>103.434784</td>\n",
              "      <td>124.578346</td>\n",
              "      <td>126.903366</td>\n",
              "      <td>126.367783</td>\n",
              "      <td>128.136612</td>\n",
              "      <td>124.749863</td>\n",
              "      <td>121.598991</td>\n",
              "      <td>121.497292</td>\n",
              "      <td>92.018021</td>\n",
              "      <td>96.648529</td>\n",
              "      <td>117.911804</td>\n",
              "      <td>148.584351</td>\n",
              "      <td>163.700699</td>\n",
              "      <td>165.060791</td>\n",
              "      <td>168.195343</td>\n",
              "      <td>171.926987</td>\n",
              "      <td>177.583054</td>\n",
              "      <td>176.192902</td>\n",
              "      <td>166.000748</td>\n",
              "      <td>...</td>\n",
              "      <td>149.693573</td>\n",
              "      <td>149.654907</td>\n",
              "      <td>150.652100</td>\n",
              "      <td>152.817230</td>\n",
              "      <td>150.598801</td>\n",
              "      <td>148.202850</td>\n",
              "      <td>147.319565</td>\n",
              "      <td>151.252762</td>\n",
              "      <td>157.825119</td>\n",
              "      <td>159.775925</td>\n",
              "      <td>156.000549</td>\n",
              "      <td>167.912354</td>\n",
              "      <td>170.747986</td>\n",
              "      <td>135.989868</td>\n",
              "      <td>133.199478</td>\n",
              "      <td>139.866013</td>\n",
              "      <td>140.996796</td>\n",
              "      <td>139.003571</td>\n",
              "      <td>138.528427</td>\n",
              "      <td>139.507599</td>\n",
              "      <td>118.837677</td>\n",
              "      <td>92.246948</td>\n",
              "      <td>99.319382</td>\n",
              "      <td>125.157623</td>\n",
              "      <td>147.388809</td>\n",
              "      <td>163.172638</td>\n",
              "      <td>165.087082</td>\n",
              "      <td>157.018387</td>\n",
              "      <td>146.574783</td>\n",
              "      <td>144.804474</td>\n",
              "      <td>150.900528</td>\n",
              "      <td>154.085007</td>\n",
              "      <td>154.106018</td>\n",
              "      <td>147.419586</td>\n",
              "      <td>142.460495</td>\n",
              "      <td>147.717773</td>\n",
              "      <td>151.873154</td>\n",
              "      <td>156.610977</td>\n",
              "      <td>157.085190</td>\n",
              "      <td>158.506454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>164.0</td>\n",
              "      <td>149.836411</td>\n",
              "      <td>148.491974</td>\n",
              "      <td>140.386078</td>\n",
              "      <td>103.972046</td>\n",
              "      <td>94.313499</td>\n",
              "      <td>127.971436</td>\n",
              "      <td>159.320053</td>\n",
              "      <td>166.696609</td>\n",
              "      <td>168.351578</td>\n",
              "      <td>163.318283</td>\n",
              "      <td>161.743607</td>\n",
              "      <td>167.306366</td>\n",
              "      <td>178.562775</td>\n",
              "      <td>190.721008</td>\n",
              "      <td>193.698975</td>\n",
              "      <td>212.133850</td>\n",
              "      <td>186.111832</td>\n",
              "      <td>113.491974</td>\n",
              "      <td>106.509216</td>\n",
              "      <td>111.108864</td>\n",
              "      <td>113.686493</td>\n",
              "      <td>118.779297</td>\n",
              "      <td>121.940521</td>\n",
              "      <td>125.341454</td>\n",
              "      <td>123.471153</td>\n",
              "      <td>120.458664</td>\n",
              "      <td>119.047600</td>\n",
              "      <td>117.195122</td>\n",
              "      <td>146.910767</td>\n",
              "      <td>142.643661</td>\n",
              "      <td>115.502678</td>\n",
              "      <td>101.876854</td>\n",
              "      <td>130.131470</td>\n",
              "      <td>161.863785</td>\n",
              "      <td>165.817368</td>\n",
              "      <td>167.646637</td>\n",
              "      <td>164.276627</td>\n",
              "      <td>160.123154</td>\n",
              "      <td>155.689468</td>\n",
              "      <td>...</td>\n",
              "      <td>157.214752</td>\n",
              "      <td>163.264114</td>\n",
              "      <td>165.566925</td>\n",
              "      <td>176.019058</td>\n",
              "      <td>168.430115</td>\n",
              "      <td>148.970840</td>\n",
              "      <td>142.895294</td>\n",
              "      <td>138.897690</td>\n",
              "      <td>136.851883</td>\n",
              "      <td>139.500290</td>\n",
              "      <td>143.251053</td>\n",
              "      <td>147.668045</td>\n",
              "      <td>227.459229</td>\n",
              "      <td>197.527649</td>\n",
              "      <td>191.882202</td>\n",
              "      <td>175.237961</td>\n",
              "      <td>168.546677</td>\n",
              "      <td>172.602615</td>\n",
              "      <td>180.919678</td>\n",
              "      <td>193.822739</td>\n",
              "      <td>218.279602</td>\n",
              "      <td>232.831055</td>\n",
              "      <td>174.037476</td>\n",
              "      <td>130.841171</td>\n",
              "      <td>148.294464</td>\n",
              "      <td>148.484833</td>\n",
              "      <td>147.820343</td>\n",
              "      <td>146.929230</td>\n",
              "      <td>149.477097</td>\n",
              "      <td>158.319458</td>\n",
              "      <td>163.980972</td>\n",
              "      <td>165.710297</td>\n",
              "      <td>158.088638</td>\n",
              "      <td>148.298035</td>\n",
              "      <td>140.710876</td>\n",
              "      <td>137.130280</td>\n",
              "      <td>139.035095</td>\n",
              "      <td>144.801315</td>\n",
              "      <td>145.669266</td>\n",
              "      <td>147.945282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>134.0</td>\n",
              "      <td>109.896652</td>\n",
              "      <td>118.946762</td>\n",
              "      <td>124.472275</td>\n",
              "      <td>127.381607</td>\n",
              "      <td>127.961464</td>\n",
              "      <td>131.841629</td>\n",
              "      <td>134.272003</td>\n",
              "      <td>135.225449</td>\n",
              "      <td>136.901306</td>\n",
              "      <td>136.563156</td>\n",
              "      <td>133.608154</td>\n",
              "      <td>130.219864</td>\n",
              "      <td>130.037430</td>\n",
              "      <td>128.778351</td>\n",
              "      <td>126.142128</td>\n",
              "      <td>131.013596</td>\n",
              "      <td>139.174652</td>\n",
              "      <td>150.058151</td>\n",
              "      <td>157.725998</td>\n",
              "      <td>159.942978</td>\n",
              "      <td>145.398987</td>\n",
              "      <td>63.433727</td>\n",
              "      <td>53.078415</td>\n",
              "      <td>56.517933</td>\n",
              "      <td>58.598801</td>\n",
              "      <td>59.202049</td>\n",
              "      <td>59.042328</td>\n",
              "      <td>58.921139</td>\n",
              "      <td>108.539536</td>\n",
              "      <td>117.830261</td>\n",
              "      <td>122.159500</td>\n",
              "      <td>124.739151</td>\n",
              "      <td>129.510132</td>\n",
              "      <td>131.764099</td>\n",
              "      <td>132.507690</td>\n",
              "      <td>135.308304</td>\n",
              "      <td>140.887939</td>\n",
              "      <td>141.732239</td>\n",
              "      <td>140.560257</td>\n",
              "      <td>...</td>\n",
              "      <td>53.582092</td>\n",
              "      <td>53.920696</td>\n",
              "      <td>54.608376</td>\n",
              "      <td>54.330811</td>\n",
              "      <td>54.910007</td>\n",
              "      <td>56.132992</td>\n",
              "      <td>56.649811</td>\n",
              "      <td>58.317448</td>\n",
              "      <td>58.952328</td>\n",
              "      <td>58.141907</td>\n",
              "      <td>56.551796</td>\n",
              "      <td>41.351528</td>\n",
              "      <td>95.748497</td>\n",
              "      <td>91.895523</td>\n",
              "      <td>88.721771</td>\n",
              "      <td>88.530632</td>\n",
              "      <td>89.844063</td>\n",
              "      <td>93.889511</td>\n",
              "      <td>97.347298</td>\n",
              "      <td>102.815781</td>\n",
              "      <td>104.539101</td>\n",
              "      <td>106.110504</td>\n",
              "      <td>102.938736</td>\n",
              "      <td>90.307877</td>\n",
              "      <td>63.918251</td>\n",
              "      <td>58.586990</td>\n",
              "      <td>57.190243</td>\n",
              "      <td>56.445312</td>\n",
              "      <td>55.072845</td>\n",
              "      <td>53.497444</td>\n",
              "      <td>52.440414</td>\n",
              "      <td>53.305191</td>\n",
              "      <td>53.555584</td>\n",
              "      <td>54.386505</td>\n",
              "      <td>56.547783</td>\n",
              "      <td>56.848518</td>\n",
              "      <td>58.826691</td>\n",
              "      <td>61.026287</td>\n",
              "      <td>61.673206</td>\n",
              "      <td>56.842281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>168.0</td>\n",
              "      <td>146.361115</td>\n",
              "      <td>156.166672</td>\n",
              "      <td>178.805557</td>\n",
              "      <td>185.083328</td>\n",
              "      <td>164.083328</td>\n",
              "      <td>165.638885</td>\n",
              "      <td>163.583328</td>\n",
              "      <td>151.611115</td>\n",
              "      <td>142.277786</td>\n",
              "      <td>142.055557</td>\n",
              "      <td>147.222229</td>\n",
              "      <td>151.305557</td>\n",
              "      <td>155.111115</td>\n",
              "      <td>155.194443</td>\n",
              "      <td>154.000000</td>\n",
              "      <td>154.527786</td>\n",
              "      <td>154.361115</td>\n",
              "      <td>146.166672</td>\n",
              "      <td>130.388885</td>\n",
              "      <td>117.111115</td>\n",
              "      <td>117.861115</td>\n",
              "      <td>127.611115</td>\n",
              "      <td>134.972229</td>\n",
              "      <td>148.555557</td>\n",
              "      <td>161.888885</td>\n",
              "      <td>163.277786</td>\n",
              "      <td>156.305557</td>\n",
              "      <td>144.972229</td>\n",
              "      <td>140.444443</td>\n",
              "      <td>146.277786</td>\n",
              "      <td>171.777786</td>\n",
              "      <td>178.861115</td>\n",
              "      <td>166.083328</td>\n",
              "      <td>157.444443</td>\n",
              "      <td>149.861115</td>\n",
              "      <td>141.666672</td>\n",
              "      <td>141.722229</td>\n",
              "      <td>142.000000</td>\n",
              "      <td>143.638885</td>\n",
              "      <td>...</td>\n",
              "      <td>148.666672</td>\n",
              "      <td>148.833328</td>\n",
              "      <td>146.555557</td>\n",
              "      <td>145.305557</td>\n",
              "      <td>145.138885</td>\n",
              "      <td>146.361115</td>\n",
              "      <td>147.805557</td>\n",
              "      <td>150.222229</td>\n",
              "      <td>151.111115</td>\n",
              "      <td>152.944443</td>\n",
              "      <td>159.611115</td>\n",
              "      <td>166.611115</td>\n",
              "      <td>161.555557</td>\n",
              "      <td>152.861115</td>\n",
              "      <td>147.111115</td>\n",
              "      <td>133.250000</td>\n",
              "      <td>121.222221</td>\n",
              "      <td>101.444443</td>\n",
              "      <td>73.972221</td>\n",
              "      <td>100.250000</td>\n",
              "      <td>113.166664</td>\n",
              "      <td>112.527779</td>\n",
              "      <td>107.638893</td>\n",
              "      <td>109.305557</td>\n",
              "      <td>119.250000</td>\n",
              "      <td>131.055557</td>\n",
              "      <td>139.527786</td>\n",
              "      <td>143.861115</td>\n",
              "      <td>148.888885</td>\n",
              "      <td>153.138885</td>\n",
              "      <td>149.416672</td>\n",
              "      <td>146.916672</td>\n",
              "      <td>146.583328</td>\n",
              "      <td>146.500000</td>\n",
              "      <td>148.194443</td>\n",
              "      <td>148.333328</td>\n",
              "      <td>149.805557</td>\n",
              "      <td>151.388885</td>\n",
              "      <td>157.833328</td>\n",
              "      <td>163.388885</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width           0           1  ...         781         782         783\n",
              "0  135.0   97.457939  114.915054  ...  155.639374  165.120438  172.667755\n",
              "1  146.0   87.688873   93.693756  ...  156.610977  157.085190  158.506454\n",
              "2  164.0  149.836411  148.491974  ...  144.801315  145.669266  147.945282\n",
              "3  134.0  109.896652  118.946762  ...   61.026287   61.673206   56.842281\n",
              "4  168.0  146.361115  156.166672  ...  151.388885  157.833328  163.388885\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC",
        "outputId": "ca412ee8-490d-499c-ed0d-6d9e51bf8828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "lost_value = float(PSD_new.columns[1])\n",
        "\n",
        "Area = np.array(PSD_new.iloc[:,1])\n",
        "Area = np.concatenate( (Area, [lost_value] ) )\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-269303394c7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlost_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPSD_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mArea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPSD_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mArea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mArea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlost_value\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdiam_teste\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Area'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aUb2_-jsY1Z"
      },
      "source": [
        "PSD_new.iloc[:,1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J705kDqsE8f"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK1GBUHWiIr4"
      },
      "source": [
        "Freq = [10.52631579, 24.21052632, 36.84210526, 14.73684211,  7.36842105, 0.]\n",
        "Freq2 = [12.90153, 28.11527, 27.66761, 20.21617, 10.34227, 0.]\n",
        "Freq3 = [22.22489, 30.15078, 25.10463, 19.30926, 2.810434, 0.]\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfk_fNXGDK5_"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nGDbBEeiUij"
      },
      "source": [
        "# plt.hist(x, bins=bins, density=True, histtype='step', cumulative=-1,label='Reversed emp.')\n",
        "plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xENlBUUxfTu"
      },
      "source": [
        "Obj = plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')\n",
        "Y1, Y2 = Obj[0]\n",
        "Rsquared = r2_score(Y1, Y2)\n",
        "print('r_squared =',Rsquared)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2XboMiFbkaa"
      },
      "source": [
        "acc_train = r.history['accuracy'][-1]\n",
        "acc_test = r.history['val_accuracy'][-1]\n",
        "loss_train = r.history['loss'][-1]\n",
        "loss_test = r.history['val_loss'][-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTd_-CYN1v0"
      },
      "source": [
        "df = pd.DataFrame({'N1':N1, 'N2':N2,'R^2':Rsquared,\n",
        "                   'acc train':acc_train,'acc test':acc_test,\n",
        "                   'loss train':loss_train,'loss test':loss_test,\n",
        "                   'Details':Description},\n",
        "                  index= [0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KukfpGTTKlj"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZHa1j4HT9Dq"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o_vDGeWUwIZ"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcH52-6iJQ8t"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r11AxFK_JIii"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xS7NSM92s_8"
      },
      "source": [
        " bins_list = [0.8, 1.0, 1.2, 1.4,1.6, 1.8,2.0]\n",
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts,bins = bins_list)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD80rFZs37Wm"
      },
      "source": [
        "yy = plt.hist(X,weights=wts,bins = bins_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edcwJoJP4GBs"
      },
      "source": [
        "yy[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0zY-NBY6Rgx"
      },
      "source": [
        "bins_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ujCjAa85RG5"
      },
      "source": [
        "k =0\n",
        "for kk in yy[0][0]:\n",
        "  name = str(bins_list[k])\n",
        "  df[name] = yy[0][1][k]\n",
        "  k = k+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMQRVn4D69RC"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMWpAwMq5Hw2"
      },
      "source": [
        "Arq = \"output.xlsx\"\n",
        "df.to_excel(Arq)\n",
        "files.download(Arq)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}