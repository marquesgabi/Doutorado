{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_ANN_r_squared_AM_8_set_9_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marquesgabi/Doutorado/blob/master/PSD_histogram_ANN_r_squared_AM_8_set_9_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b125783-709e-4bed-bad6-471163689369"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mahotas\n",
            "  Downloading mahotas-1.4.11-cp37-cp37m-manylinux2010_x86_64.whl (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d3d3543-9ecf-4712-8b37-d87e55d9ea45"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823a298c-619a-4e12-82a8-908d4285b0da"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 443, done.\u001b[K\n",
            "remote: Counting objects: 100% (193/193), done.\u001b[K\n",
            "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
            "remote: Total 443 (delta 88), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (443/443), 165.85 MiB | 30.74 MiB/s, done.\n",
            "Resolving deltas: 100% (211/211), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[9] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c636a650-7261-41f3-9811-2cca34c3c8a8"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 22.84 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad04d4b-683a-4891-8fc3-5fada99f5db1"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     135   58.875549   58.810150  ...    6.206200    2.624472    1.199726\n",
            "1     137   79.889610   80.594704  ...   93.303909   91.565887   92.672806\n",
            "2     118  105.454170  106.303360  ...   81.729965   84.274628   86.985336\n",
            "3     147   98.587311   95.775520  ...   51.503399   51.394558   53.253967\n",
            "4     111   87.968918   90.552795  ...   79.332443   79.644989   80.379669\n",
            "5     105   50.075562   46.208897  ...   60.977783   54.960007   56.560005\n",
            "6     195   73.237343   81.286797  ...    5.921026    7.278896    5.938830\n",
            "7     117   80.285416   76.000656  ...   55.457886   57.947693   59.556576\n",
            "8     142   50.430271   50.216423  ...   47.255505   45.246181   44.894661\n",
            "9     141   56.466274   62.195969  ...   59.679596   66.809319   75.020668\n",
            "10    193   41.658485   42.133720  ...   24.528175   21.347984   11.954334\n",
            "11    185    0.523302    5.634769  ...    6.105858    5.509452    5.856333\n",
            "12    120   66.983330   66.614449  ...   95.044449   99.338898  100.991119\n",
            "13    102   49.201462   48.104195  ...   76.805847   73.943489   70.513275\n",
            "14    181    2.016269    1.847776  ...  113.772049  104.917984   21.155582\n",
            "15    147   53.827671   65.850342  ...   67.376427   67.952385   68.850349\n",
            "16    151  100.922340   93.698082  ...  108.293106  111.502747  114.335335\n",
            "17    140   46.399998   50.360001  ...   61.439999   63.480000   65.000000\n",
            "18    190   60.271797   66.028481  ...   80.356339   74.606865   74.177841\n",
            "19    100   79.449600   79.609596  ...   83.532799   82.092804   81.259201\n",
            "20    200   84.868805   84.388809  ...   65.675201   68.869598   69.686401\n",
            "21    146   71.934319   71.981239  ...    9.576281   19.166071   29.510601\n",
            "22    171    0.157348    0.586950  ...   74.937073   62.420467    9.258883\n",
            "23    162   66.392319   78.781281  ...   58.088402   58.384544   60.692726\n",
            "24    143   66.273659   76.121277  ...   63.698807   65.799355   83.080589\n",
            "25    129   77.820564   78.334717  ...   48.064301   49.503216   50.482361\n",
            "26    166   23.655828   16.016693  ...    4.842648    4.605458    2.900131\n",
            "27    116  101.420921   96.932228  ...    8.344828    8.017836    8.240191\n",
            "28    150   69.843201   71.398048  ...   32.538486   30.198221   36.707733\n",
            "29    109   71.065231   72.007233  ...   62.310665   54.346603   32.082993\n",
            "30    185   73.418602   78.185364  ...   49.851162   33.567245   21.036230\n",
            "31    138   65.425537   36.773155  ...   71.751945   63.605331   56.814323\n",
            "32    133   63.886429   69.066483  ...    7.614959    7.313019    7.750692\n",
            "33    103   50.647186   43.413700  ...   79.294273   83.985855   83.985672\n",
            "34    186   28.759396   53.066715  ...  104.348030  105.342827  105.267914\n",
            "35    124   74.574394   70.618103  ...   40.197708   52.361080   53.894897\n",
            "36    172   59.338562   58.083832  ...   51.478100   62.090321   61.670635\n",
            "37    189   44.654320   45.864197  ...    0.000000    0.000000    0.000000\n",
            "38    182   50.207100   46.597630  ...   17.863905   10.331362    1.207101\n",
            "39    126  103.777786  104.074074  ...   75.641975   79.037041   79.259262\n",
            "40    138  128.155640  122.867249  ...   91.001259   93.087791   93.946854\n",
            "41    188   70.330467   75.440018  ...    7.853327    6.076958    5.750113\n",
            "42    127    1.500155    3.106330  ...   45.756588   40.087296   38.901108\n",
            "43    152   82.515228   85.691826  ...   87.389885   86.441132   83.002769\n",
            "44    169   36.462868   39.182312  ...    6.885087    7.751688    6.806694\n",
            "45    145   48.568081   51.619026  ...    7.272533    7.118050    7.016837\n",
            "46    120  101.458893  100.780006  ...   54.382221   63.445557   67.963333\n",
            "47    132   86.240593   86.624435  ...  168.564743  171.992661  178.258942\n",
            "48    153   50.667480   55.000088  ...   44.328506   48.225769   53.760651\n",
            "49    134   88.895081   89.379372  ...   65.965248   56.434395   48.143906\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9039751b-e709-46d7-d195-2cbcd9405d33"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "cedf8a33-ad21-4d6c-a1fd-145255a49ad1"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 22.92 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = 'ANN without convolution '\n",
        "N1 = 200\n",
        "N2 = 10\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "e12c48fd-0cb2-4c59-ee65-2cf873b6626e"
      },
      "source": [
        "\n",
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 25ms/step - loss: 0.6936 - accuracy: 0.5044 - val_loss: 0.6913 - val_accuracy: 0.5102\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.6901 - accuracy: 0.6064 - val_loss: 0.6879 - val_accuracy: 0.6667\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.6881 - accuracy: 0.6618 - val_loss: 0.6846 - val_accuracy: 0.7415\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6833 - accuracy: 0.6822 - val_loss: 0.6786 - val_accuracy: 0.7211\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6765 - accuracy: 0.6880 - val_loss: 0.6715 - val_accuracy: 0.7619\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6695 - accuracy: 0.6910 - val_loss: 0.6614 - val_accuracy: 0.7279\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.6591 - accuracy: 0.7405 - val_loss: 0.6493 - val_accuracy: 0.7619\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.6485 - accuracy: 0.7201 - val_loss: 0.6354 - val_accuracy: 0.7415\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6433 - accuracy: 0.7289 - val_loss: 0.6243 - val_accuracy: 0.7687\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6236 - accuracy: 0.7376 - val_loss: 0.6053 - val_accuracy: 0.7483\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6044 - accuracy: 0.7638 - val_loss: 0.5893 - val_accuracy: 0.7755\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6033 - accuracy: 0.7493 - val_loss: 0.5677 - val_accuracy: 0.7687\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5774 - accuracy: 0.7580 - val_loss: 0.5580 - val_accuracy: 0.7823\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.5619 - accuracy: 0.7668 - val_loss: 0.5315 - val_accuracy: 0.7891\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.5532 - accuracy: 0.7551 - val_loss: 0.5176 - val_accuracy: 0.7891\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.5331 - accuracy: 0.7697 - val_loss: 0.4965 - val_accuracy: 0.8299\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5108 - accuracy: 0.7930 - val_loss: 0.4768 - val_accuracy: 0.8367\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4936 - accuracy: 0.8076 - val_loss: 0.4568 - val_accuracy: 0.8571\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.4910 - accuracy: 0.8047 - val_loss: 0.4469 - val_accuracy: 0.8367\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4840 - accuracy: 0.7988 - val_loss: 0.4218 - val_accuracy: 0.8707\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.4617 - accuracy: 0.8309 - val_loss: 0.4096 - val_accuracy: 0.8844\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4449 - accuracy: 0.8426 - val_loss: 0.3921 - val_accuracy: 0.8912\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.4269 - accuracy: 0.8280 - val_loss: 0.3737 - val_accuracy: 0.9116\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3907 - accuracy: 0.8688 - val_loss: 0.3709 - val_accuracy: 0.9048\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.4042 - accuracy: 0.8367 - val_loss: 0.3448 - val_accuracy: 0.9320\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3788 - accuracy: 0.8776 - val_loss: 0.3318 - val_accuracy: 0.9388\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3952 - accuracy: 0.8251 - val_loss: 0.3257 - val_accuracy: 0.9048\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3700 - accuracy: 0.8513 - val_loss: 0.3100 - val_accuracy: 0.9456\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3722 - accuracy: 0.8717 - val_loss: 0.3048 - val_accuracy: 0.9252\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3721 - accuracy: 0.8484 - val_loss: 0.2959 - val_accuracy: 0.9388\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3811 - accuracy: 0.8542 - val_loss: 0.2939 - val_accuracy: 0.9116\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3559 - accuracy: 0.8717 - val_loss: 0.2804 - val_accuracy: 0.9252\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3393 - accuracy: 0.8776 - val_loss: 0.2846 - val_accuracy: 0.9116\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3465 - accuracy: 0.8659 - val_loss: 0.2630 - val_accuracy: 0.9320\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3141 - accuracy: 0.8950 - val_loss: 0.2580 - val_accuracy: 0.9456\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3212 - accuracy: 0.8834 - val_loss: 0.2545 - val_accuracy: 0.9388\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3021 - accuracy: 0.8892 - val_loss: 0.2449 - val_accuracy: 0.9320\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2983 - accuracy: 0.8863 - val_loss: 0.2446 - val_accuracy: 0.9388\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.3065 - accuracy: 0.8717 - val_loss: 0.2342 - val_accuracy: 0.9388\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3086 - accuracy: 0.8746 - val_loss: 0.2373 - val_accuracy: 0.9388\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2871 - accuracy: 0.8950 - val_loss: 0.2288 - val_accuracy: 0.9456\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2757 - accuracy: 0.8980 - val_loss: 0.2261 - val_accuracy: 0.9320\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2913 - accuracy: 0.8892 - val_loss: 0.2428 - val_accuracy: 0.9184\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3001 - accuracy: 0.8834 - val_loss: 0.2267 - val_accuracy: 0.9252\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.2942 - accuracy: 0.8892 - val_loss: 0.2312 - val_accuracy: 0.9320\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2691 - accuracy: 0.8980 - val_loss: 0.2097 - val_accuracy: 0.9320\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2686 - accuracy: 0.8892 - val_loss: 0.2060 - val_accuracy: 0.9320\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3069 - accuracy: 0.8776 - val_loss: 0.2244 - val_accuracy: 0.9320\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3102 - accuracy: 0.8776 - val_loss: 0.2046 - val_accuracy: 0.9320\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2988 - accuracy: 0.8659 - val_loss: 0.2070 - val_accuracy: 0.9388\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2592 - accuracy: 0.9125 - val_loss: 0.1997 - val_accuracy: 0.9524\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2480 - accuracy: 0.9096 - val_loss: 0.1979 - val_accuracy: 0.9456\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2555 - accuracy: 0.9155 - val_loss: 0.1939 - val_accuracy: 0.9524\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2725 - accuracy: 0.8980 - val_loss: 0.1995 - val_accuracy: 0.9456\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2516 - accuracy: 0.8950 - val_loss: 0.1927 - val_accuracy: 0.9252\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2604 - accuracy: 0.9184 - val_loss: 0.2137 - val_accuracy: 0.9320\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2924 - accuracy: 0.8834 - val_loss: 0.1893 - val_accuracy: 0.9252\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2541 - accuracy: 0.9009 - val_loss: 0.1997 - val_accuracy: 0.9388\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2542 - accuracy: 0.8980 - val_loss: 0.1837 - val_accuracy: 0.9320\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2259 - accuracy: 0.9184 - val_loss: 0.1845 - val_accuracy: 0.9388\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2324 - accuracy: 0.9213 - val_loss: 0.1792 - val_accuracy: 0.9320\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2336 - accuracy: 0.9155 - val_loss: 0.1825 - val_accuracy: 0.9388\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2433 - accuracy: 0.9067 - val_loss: 0.1760 - val_accuracy: 0.9320\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2310 - accuracy: 0.9096 - val_loss: 0.1763 - val_accuracy: 0.9456\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2477 - accuracy: 0.9009 - val_loss: 0.1817 - val_accuracy: 0.9456\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2450 - accuracy: 0.9009 - val_loss: 0.1759 - val_accuracy: 0.9320\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2202 - accuracy: 0.9242 - val_loss: 0.1770 - val_accuracy: 0.9456\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.2318 - accuracy: 0.9242 - val_loss: 0.1692 - val_accuracy: 0.9388\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.2125 - accuracy: 0.9300 - val_loss: 0.1726 - val_accuracy: 0.9388\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2291 - accuracy: 0.9184 - val_loss: 0.1666 - val_accuracy: 0.9388\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1928 - accuracy: 0.9329 - val_loss: 0.1725 - val_accuracy: 0.9388\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2364 - accuracy: 0.9155 - val_loss: 0.1647 - val_accuracy: 0.9388\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.2175 - accuracy: 0.9300 - val_loss: 0.1655 - val_accuracy: 0.9456\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2208 - accuracy: 0.9242 - val_loss: 0.1652 - val_accuracy: 0.9320\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2241 - accuracy: 0.9242 - val_loss: 0.1632 - val_accuracy: 0.9456\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2107 - accuracy: 0.9242 - val_loss: 0.1674 - val_accuracy: 0.9320\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.2374 - accuracy: 0.9096 - val_loss: 0.1929 - val_accuracy: 0.9252\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2231 - accuracy: 0.9184 - val_loss: 0.1611 - val_accuracy: 0.9388\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2193 - accuracy: 0.9242 - val_loss: 0.1588 - val_accuracy: 0.9524\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2146 - accuracy: 0.9242 - val_loss: 0.1570 - val_accuracy: 0.9456\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1910 - accuracy: 0.9213 - val_loss: 0.1553 - val_accuracy: 0.9592\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1962 - accuracy: 0.9388 - val_loss: 0.1531 - val_accuracy: 0.9660\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2023 - accuracy: 0.9242 - val_loss: 0.1564 - val_accuracy: 0.9456\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1933 - accuracy: 0.9300 - val_loss: 0.1517 - val_accuracy: 0.9660\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1946 - accuracy: 0.9242 - val_loss: 0.1497 - val_accuracy: 0.9592\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2350 - accuracy: 0.9009 - val_loss: 0.1495 - val_accuracy: 0.9456\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.2128 - accuracy: 0.9184 - val_loss: 0.1568 - val_accuracy: 0.9456\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2155 - accuracy: 0.9242 - val_loss: 0.1503 - val_accuracy: 0.9388\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2022 - accuracy: 0.9213 - val_loss: 0.1579 - val_accuracy: 0.9456\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1880 - accuracy: 0.9271 - val_loss: 0.1467 - val_accuracy: 0.9456\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1959 - accuracy: 0.9242 - val_loss: 0.1458 - val_accuracy: 0.9660\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1951 - accuracy: 0.9446 - val_loss: 0.1440 - val_accuracy: 0.9660\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1904 - accuracy: 0.9242 - val_loss: 0.1430 - val_accuracy: 0.9592\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1891 - accuracy: 0.9300 - val_loss: 0.1432 - val_accuracy: 0.9456\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1940 - accuracy: 0.9213 - val_loss: 0.1426 - val_accuracy: 0.9660\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1734 - accuracy: 0.9417 - val_loss: 0.1416 - val_accuracy: 0.9660\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1795 - accuracy: 0.9329 - val_loss: 0.1420 - val_accuracy: 0.9456\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2053 - accuracy: 0.9329 - val_loss: 0.1461 - val_accuracy: 0.9592\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1919 - accuracy: 0.9388 - val_loss: 0.1427 - val_accuracy: 0.9592\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1799 - accuracy: 0.9300 - val_loss: 0.1421 - val_accuracy: 0.9320\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2063 - accuracy: 0.9300 - val_loss: 0.1435 - val_accuracy: 0.9592\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1731 - accuracy: 0.9417 - val_loss: 0.1365 - val_accuracy: 0.9660\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1879 - accuracy: 0.9242 - val_loss: 0.1401 - val_accuracy: 0.9592\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1767 - accuracy: 0.9359 - val_loss: 0.1372 - val_accuracy: 0.9320\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1749 - accuracy: 0.9388 - val_loss: 0.1338 - val_accuracy: 0.9592\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1633 - accuracy: 0.9475 - val_loss: 0.1314 - val_accuracy: 0.9660\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1819 - accuracy: 0.9300 - val_loss: 0.1338 - val_accuracy: 0.9592\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1785 - accuracy: 0.9329 - val_loss: 0.1311 - val_accuracy: 0.9660\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1556 - accuracy: 0.9563 - val_loss: 0.1297 - val_accuracy: 0.9660\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1752 - accuracy: 0.9359 - val_loss: 0.1322 - val_accuracy: 0.9592\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1685 - accuracy: 0.9329 - val_loss: 0.1285 - val_accuracy: 0.9592\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1712 - accuracy: 0.9475 - val_loss: 0.1284 - val_accuracy: 0.9660\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1724 - accuracy: 0.9359 - val_loss: 0.1266 - val_accuracy: 0.9660\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1735 - accuracy: 0.9300 - val_loss: 0.1260 - val_accuracy: 0.9660\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1791 - accuracy: 0.9417 - val_loss: 0.1246 - val_accuracy: 0.9660\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1914 - accuracy: 0.9184 - val_loss: 0.1247 - val_accuracy: 0.9660\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1588 - accuracy: 0.9592 - val_loss: 0.1245 - val_accuracy: 0.9660\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1659 - accuracy: 0.9475 - val_loss: 0.1267 - val_accuracy: 0.9592\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1741 - accuracy: 0.9417 - val_loss: 0.1239 - val_accuracy: 0.9660\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1593 - accuracy: 0.9504 - val_loss: 0.1260 - val_accuracy: 0.9524\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1595 - accuracy: 0.9504 - val_loss: 0.1275 - val_accuracy: 0.9388\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1664 - accuracy: 0.9359 - val_loss: 0.1299 - val_accuracy: 0.9592\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1605 - accuracy: 0.9475 - val_loss: 0.1207 - val_accuracy: 0.9660\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1466 - accuracy: 0.9417 - val_loss: 0.1308 - val_accuracy: 0.9592\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1537 - accuracy: 0.9504 - val_loss: 0.1176 - val_accuracy: 0.9660\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1550 - accuracy: 0.9475 - val_loss: 0.1172 - val_accuracy: 0.9592\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1722 - accuracy: 0.9388 - val_loss: 0.1168 - val_accuracy: 0.9592\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1511 - accuracy: 0.9621 - val_loss: 0.1158 - val_accuracy: 0.9660\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1556 - accuracy: 0.9621 - val_loss: 0.1148 - val_accuracy: 0.9660\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1587 - accuracy: 0.9621 - val_loss: 0.1176 - val_accuracy: 0.9524\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1444 - accuracy: 0.9446 - val_loss: 0.1137 - val_accuracy: 0.9592\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1491 - accuracy: 0.9446 - val_loss: 0.1131 - val_accuracy: 0.9592\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1566 - accuracy: 0.9592 - val_loss: 0.1157 - val_accuracy: 0.9524\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1493 - accuracy: 0.9475 - val_loss: 0.1129 - val_accuracy: 0.9592\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1514 - accuracy: 0.9563 - val_loss: 0.1152 - val_accuracy: 0.9592\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1482 - accuracy: 0.9534 - val_loss: 0.1134 - val_accuracy: 0.9524\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1391 - accuracy: 0.9534 - val_loss: 0.1148 - val_accuracy: 0.9456\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1600 - accuracy: 0.9534 - val_loss: 0.1254 - val_accuracy: 0.9524\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1661 - accuracy: 0.9271 - val_loss: 0.1077 - val_accuracy: 0.9660\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1455 - accuracy: 0.9534 - val_loss: 0.1086 - val_accuracy: 0.9592\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1464 - accuracy: 0.9417 - val_loss: 0.1096 - val_accuracy: 0.9592\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1399 - accuracy: 0.9534 - val_loss: 0.1085 - val_accuracy: 0.9592\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1427 - accuracy: 0.9563 - val_loss: 0.1073 - val_accuracy: 0.9592\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1495 - accuracy: 0.9504 - val_loss: 0.1067 - val_accuracy: 0.9592\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1435 - accuracy: 0.9621 - val_loss: 0.1085 - val_accuracy: 0.9592\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1437 - accuracy: 0.9650 - val_loss: 0.1060 - val_accuracy: 0.9592\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1225 - accuracy: 0.9679 - val_loss: 0.1055 - val_accuracy: 0.9592\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1363 - accuracy: 0.9592 - val_loss: 0.1117 - val_accuracy: 0.9456\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1284 - accuracy: 0.9592 - val_loss: 0.1091 - val_accuracy: 0.9592\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1253 - accuracy: 0.9650 - val_loss: 0.1028 - val_accuracy: 0.9592\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1255 - accuracy: 0.9621 - val_loss: 0.1027 - val_accuracy: 0.9592\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1413 - accuracy: 0.9534 - val_loss: 0.1025 - val_accuracy: 0.9592\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1395 - accuracy: 0.9592 - val_loss: 0.1076 - val_accuracy: 0.9592\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1360 - accuracy: 0.9650 - val_loss: 0.1018 - val_accuracy: 0.9592\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1318 - accuracy: 0.9650 - val_loss: 0.1006 - val_accuracy: 0.9592\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1272 - accuracy: 0.9563 - val_loss: 0.1012 - val_accuracy: 0.9592\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1252 - accuracy: 0.9650 - val_loss: 0.1028 - val_accuracy: 0.9592\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1302 - accuracy: 0.9563 - val_loss: 0.1026 - val_accuracy: 0.9524\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1330 - accuracy: 0.9592 - val_loss: 0.1057 - val_accuracy: 0.9592\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1247 - accuracy: 0.9534 - val_loss: 0.0978 - val_accuracy: 0.9592\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1154 - accuracy: 0.9650 - val_loss: 0.1080 - val_accuracy: 0.9388\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1343 - accuracy: 0.9446 - val_loss: 0.1177 - val_accuracy: 0.9592\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1335 - accuracy: 0.9446 - val_loss: 0.0996 - val_accuracy: 0.9524\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1280 - accuracy: 0.9592 - val_loss: 0.0977 - val_accuracy: 0.9592\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1249 - accuracy: 0.9650 - val_loss: 0.0985 - val_accuracy: 0.9592\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1244 - accuracy: 0.9621 - val_loss: 0.0989 - val_accuracy: 0.9592\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1258 - accuracy: 0.9504 - val_loss: 0.0988 - val_accuracy: 0.9592\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1230 - accuracy: 0.9738 - val_loss: 0.0977 - val_accuracy: 0.9592\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1165 - accuracy: 0.9679 - val_loss: 0.1005 - val_accuracy: 0.9660\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0973 - accuracy: 0.9796 - val_loss: 0.0983 - val_accuracy: 0.9524\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1046 - accuracy: 0.9679 - val_loss: 0.0947 - val_accuracy: 0.9592\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1050 - accuracy: 0.9767 - val_loss: 0.0939 - val_accuracy: 0.9592\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1142 - accuracy: 0.9621 - val_loss: 0.0926 - val_accuracy: 0.9592\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1229 - accuracy: 0.9679 - val_loss: 0.0952 - val_accuracy: 0.9524\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1266 - accuracy: 0.9563 - val_loss: 0.1031 - val_accuracy: 0.9592\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1114 - accuracy: 0.9650 - val_loss: 0.1001 - val_accuracy: 0.9592\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1200 - accuracy: 0.9650 - val_loss: 0.0972 - val_accuracy: 0.9660\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1170 - accuracy: 0.9592 - val_loss: 0.0951 - val_accuracy: 0.9660\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.1172 - accuracy: 0.9650 - val_loss: 0.0927 - val_accuracy: 0.9592\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1139 - accuracy: 0.9621 - val_loss: 0.0944 - val_accuracy: 0.9524\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0974 - accuracy: 0.9679 - val_loss: 0.1048 - val_accuracy: 0.9660\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1144 - accuracy: 0.9708 - val_loss: 0.1119 - val_accuracy: 0.9456\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1308 - accuracy: 0.9592 - val_loss: 0.1158 - val_accuracy: 0.9660\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1178 - accuracy: 0.9621 - val_loss: 0.0990 - val_accuracy: 0.9524\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1023 - accuracy: 0.9621 - val_loss: 0.0924 - val_accuracy: 0.9592\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1128 - accuracy: 0.9621 - val_loss: 0.0957 - val_accuracy: 0.9660\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1082 - accuracy: 0.9592 - val_loss: 0.1024 - val_accuracy: 0.9456\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1147 - accuracy: 0.9592 - val_loss: 0.0942 - val_accuracy: 0.9660\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1004 - accuracy: 0.9592 - val_loss: 0.1025 - val_accuracy: 0.9456\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1064 - accuracy: 0.9592 - val_loss: 0.0900 - val_accuracy: 0.9592\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0948 - accuracy: 0.9767 - val_loss: 0.0884 - val_accuracy: 0.9592\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0959 - accuracy: 0.9796 - val_loss: 0.0874 - val_accuracy: 0.9592\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1006 - accuracy: 0.9650 - val_loss: 0.0874 - val_accuracy: 0.9592\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1109 - accuracy: 0.9679 - val_loss: 0.0885 - val_accuracy: 0.9660\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1003 - accuracy: 0.9679 - val_loss: 0.0888 - val_accuracy: 0.9660\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0972 - accuracy: 0.9767 - val_loss: 0.0925 - val_accuracy: 0.9592\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1225 - accuracy: 0.9534 - val_loss: 0.1353 - val_accuracy: 0.9456\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1406 - accuracy: 0.9446 - val_loss: 0.0960 - val_accuracy: 0.9592\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1158 - accuracy: 0.9534 - val_loss: 0.1005 - val_accuracy: 0.9456\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0959 - accuracy: 0.9738 - val_loss: 0.0921 - val_accuracy: 0.9660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJTKDERlGBIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "960c910f-6e96-4c8e-f5da-769fce431fe2"
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        70   2\n",
            "1         3  72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iFNNrlWV9tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0efc2d2-8df9-4ce2-b23f-99356ef675d3"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNDIL0I5HBzK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3dbfc43-f8a7-425f-a04a-0586f8180ba7"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[9] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  #prediction = model.predict_classes(result)\n",
        "  prediction = np.argmax(model.predict(result), axis=-1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "3   192.0   74.573349   77.677505  ...  100.246094   55.891491   49.351994\n",
            "4   137.0   40.407211   44.869568  ...   87.582230   86.839935   86.061630\n",
            "5   139.0   88.045952   81.073753  ...   16.541586   16.514517   15.995704\n",
            "7   158.0   75.679695   55.978371  ...    0.000000    0.000000    0.000000\n",
            "8   142.0   54.219994   42.878597  ...   38.976395   23.533625   29.693117\n",
            "10  178.0   71.768845   73.244919  ...    5.941801    6.765434    6.210454\n",
            "11  136.0   81.518173   88.492218  ...   60.603806   60.918690   62.653114\n",
            "13  117.0   54.607498   54.849152  ...   61.864128   61.483234   59.078678\n",
            "14  191.0   25.335930   26.591705  ...   80.955132   82.034554   79.374306\n",
            "15  192.0   17.044703   14.112846  ...   79.866753   85.597656   90.280380\n",
            "19  157.0   62.032578   66.206055  ...   92.578247   88.909897   89.042397\n",
            "20  169.0   79.714851   82.964813  ...    0.085291    0.000000    0.000000\n",
            "22  155.0   59.089703   64.072098  ...   83.979362   77.712891   54.833961\n",
            "24  155.0   52.805374   51.250576  ...   56.562794   55.672676   52.335651\n",
            "26  148.0   43.000004   36.140980  ...   21.774288   16.735575    8.247627\n",
            "27  168.0   56.305557   58.083332  ...   58.611111   57.250000   58.750000\n",
            "30  102.0   23.800848   37.526340  ...   64.728577   65.397934   64.618996\n",
            "31  127.0  145.411255  140.380188  ...   94.841087   91.342422   85.604324\n",
            "33  105.0   39.657780   40.022228  ...   31.133335   29.782225   25.328892\n",
            "34  184.0   46.008030   42.710773  ...   69.890358   69.913513   66.551979\n",
            "35  173.0   81.559349   82.889305  ...    0.000000    0.000000    0.000000\n",
            "36  188.0   73.987328   76.685829  ...   72.451332   75.463562   79.569946\n",
            "37  196.0  100.673470  100.183670  ...   69.346939   76.020409   81.306122\n",
            "39  138.0   49.487923   49.409786  ...   55.415668   56.336063   57.470276\n",
            "40  180.0  129.127899   71.190620  ...   79.772354   80.535309   81.892349\n",
            "42  196.0   81.857140   82.755104  ...   61.183674   62.102039   65.183670\n",
            "43  119.0   46.906574   43.148788  ...  121.401382  123.079582  125.377167\n",
            "44  130.0   33.043076   25.276924  ...    8.057988    1.779408    0.528757\n",
            "45  176.0   65.390495   67.283051  ...    0.000000    0.941116    0.920971\n",
            "46  141.0   41.059704   44.882149  ...   36.973396   22.483627   14.248328\n",
            "47  137.0   91.747398   90.616173  ...    7.537109    7.095583    7.010549\n",
            "48  199.0   63.562832   58.467861  ...   10.470720    5.620135    5.115754\n",
            "49  139.0   15.340872   53.463120  ...   84.970123   92.205261   94.316017\n",
            "\n",
            "[33 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjRbWgmX_LFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4aeea85-05cf-4ee3-8434-91a7194299b7"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 687, done.\u001b[K\n",
            "remote: Counting objects: 100% (448/448), done.\u001b[K\n",
            "remote: Compressing objects: 100% (446/446), done.\u001b[K\n",
            "remote: Total 687 (delta 282), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (687/687), 5.59 MiB | 12.29 MiB/s, done.\n",
            "Resolving deltas: 100% (419/419), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6ckWQjGGfa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b2cfbe2-d0ce-4940-f9cb-c5c9d9c67ab6"
      },
      "source": [
        "#!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "\n",
        "%cd Doutorado\n",
        "\n",
        "PSD_imageJ = 'Amostra8.csv' \n",
        "PSD_new = pd.read_csv(PSD_imageJ,sep=';')\n",
        "#encoding='utf8'\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 443, done.\u001b[K\n",
            "remote: Counting objects: 100% (193/193), done.\u001b[K\n",
            "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
            "remote: Total 443 (delta 88), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (443/443), 165.85 MiB | 28.85 MiB/s, done.\n",
            "Resolving deltas: 100% (211/211), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/Doutorado\n",
            "   Unnamed: 0   Area\n",
            "0           1  0.807\n",
            "1           2  1.407\n",
            "2           3  1.177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PekBHQOT_6CP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "0d8c8f19-1bb2-45ed-8105-49369086c8c1"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>192.0</td>\n",
              "      <td>74.573349</td>\n",
              "      <td>77.677505</td>\n",
              "      <td>78.432724</td>\n",
              "      <td>80.206589</td>\n",
              "      <td>77.560326</td>\n",
              "      <td>76.581154</td>\n",
              "      <td>72.816833</td>\n",
              "      <td>72.582893</td>\n",
              "      <td>72.168396</td>\n",
              "      <td>61.186630</td>\n",
              "      <td>48.788628</td>\n",
              "      <td>50.444008</td>\n",
              "      <td>51.877167</td>\n",
              "      <td>53.442707</td>\n",
              "      <td>74.922302</td>\n",
              "      <td>90.710503</td>\n",
              "      <td>95.646690</td>\n",
              "      <td>95.687927</td>\n",
              "      <td>94.633240</td>\n",
              "      <td>96.402336</td>\n",
              "      <td>97.397560</td>\n",
              "      <td>97.375427</td>\n",
              "      <td>97.554680</td>\n",
              "      <td>101.928810</td>\n",
              "      <td>104.666222</td>\n",
              "      <td>105.944000</td>\n",
              "      <td>109.572037</td>\n",
              "      <td>103.667099</td>\n",
              "      <td>77.820305</td>\n",
              "      <td>81.908852</td>\n",
              "      <td>81.564240</td>\n",
              "      <td>80.034714</td>\n",
              "      <td>80.736107</td>\n",
              "      <td>78.995216</td>\n",
              "      <td>79.277771</td>\n",
              "      <td>81.134537</td>\n",
              "      <td>87.720482</td>\n",
              "      <td>82.864143</td>\n",
              "      <td>57.186199</td>\n",
              "      <td>...</td>\n",
              "      <td>101.392792</td>\n",
              "      <td>102.793396</td>\n",
              "      <td>108.763443</td>\n",
              "      <td>103.886276</td>\n",
              "      <td>101.011276</td>\n",
              "      <td>102.930122</td>\n",
              "      <td>104.469612</td>\n",
              "      <td>108.322037</td>\n",
              "      <td>117.586800</td>\n",
              "      <td>124.422295</td>\n",
              "      <td>83.579422</td>\n",
              "      <td>44.856770</td>\n",
              "      <td>84.876297</td>\n",
              "      <td>88.116745</td>\n",
              "      <td>89.534721</td>\n",
              "      <td>94.628899</td>\n",
              "      <td>102.756065</td>\n",
              "      <td>113.047737</td>\n",
              "      <td>116.767365</td>\n",
              "      <td>102.876732</td>\n",
              "      <td>74.157982</td>\n",
              "      <td>57.010414</td>\n",
              "      <td>55.680119</td>\n",
              "      <td>56.575954</td>\n",
              "      <td>59.109810</td>\n",
              "      <td>59.630203</td>\n",
              "      <td>67.492180</td>\n",
              "      <td>93.402344</td>\n",
              "      <td>101.942703</td>\n",
              "      <td>102.261711</td>\n",
              "      <td>104.207893</td>\n",
              "      <td>102.151909</td>\n",
              "      <td>99.584198</td>\n",
              "      <td>100.338112</td>\n",
              "      <td>102.503899</td>\n",
              "      <td>107.407547</td>\n",
              "      <td>113.878021</td>\n",
              "      <td>100.246094</td>\n",
              "      <td>55.891491</td>\n",
              "      <td>49.351994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>137.0</td>\n",
              "      <td>40.407211</td>\n",
              "      <td>44.869568</td>\n",
              "      <td>47.756832</td>\n",
              "      <td>51.005169</td>\n",
              "      <td>58.022163</td>\n",
              "      <td>62.836590</td>\n",
              "      <td>65.048477</td>\n",
              "      <td>67.477386</td>\n",
              "      <td>67.841064</td>\n",
              "      <td>69.317757</td>\n",
              "      <td>70.552185</td>\n",
              "      <td>69.833443</td>\n",
              "      <td>69.552025</td>\n",
              "      <td>67.638443</td>\n",
              "      <td>65.399857</td>\n",
              "      <td>62.957748</td>\n",
              "      <td>61.251102</td>\n",
              "      <td>63.193989</td>\n",
              "      <td>66.540520</td>\n",
              "      <td>74.065056</td>\n",
              "      <td>79.213867</td>\n",
              "      <td>79.746979</td>\n",
              "      <td>80.893066</td>\n",
              "      <td>80.037819</td>\n",
              "      <td>74.412491</td>\n",
              "      <td>70.811333</td>\n",
              "      <td>65.066544</td>\n",
              "      <td>62.484306</td>\n",
              "      <td>39.523094</td>\n",
              "      <td>45.059563</td>\n",
              "      <td>47.985188</td>\n",
              "      <td>50.451378</td>\n",
              "      <td>56.977356</td>\n",
              "      <td>62.982677</td>\n",
              "      <td>65.506264</td>\n",
              "      <td>68.275932</td>\n",
              "      <td>68.869148</td>\n",
              "      <td>68.822685</td>\n",
              "      <td>68.116570</td>\n",
              "      <td>...</td>\n",
              "      <td>90.634285</td>\n",
              "      <td>90.805954</td>\n",
              "      <td>92.154022</td>\n",
              "      <td>93.352486</td>\n",
              "      <td>94.625809</td>\n",
              "      <td>93.458893</td>\n",
              "      <td>89.694176</td>\n",
              "      <td>86.666100</td>\n",
              "      <td>86.966751</td>\n",
              "      <td>88.445145</td>\n",
              "      <td>88.303108</td>\n",
              "      <td>87.836388</td>\n",
              "      <td>82.281479</td>\n",
              "      <td>83.083969</td>\n",
              "      <td>83.809044</td>\n",
              "      <td>84.110817</td>\n",
              "      <td>86.836487</td>\n",
              "      <td>88.757324</td>\n",
              "      <td>87.914055</td>\n",
              "      <td>89.631897</td>\n",
              "      <td>90.083702</td>\n",
              "      <td>88.482010</td>\n",
              "      <td>87.776123</td>\n",
              "      <td>88.332993</td>\n",
              "      <td>87.319778</td>\n",
              "      <td>91.775528</td>\n",
              "      <td>93.274284</td>\n",
              "      <td>91.649796</td>\n",
              "      <td>88.196106</td>\n",
              "      <td>86.497253</td>\n",
              "      <td>89.256699</td>\n",
              "      <td>93.364639</td>\n",
              "      <td>94.041763</td>\n",
              "      <td>93.601364</td>\n",
              "      <td>93.198349</td>\n",
              "      <td>90.561340</td>\n",
              "      <td>88.847725</td>\n",
              "      <td>87.582230</td>\n",
              "      <td>86.839935</td>\n",
              "      <td>86.061630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>139.0</td>\n",
              "      <td>88.045952</td>\n",
              "      <td>81.073753</td>\n",
              "      <td>75.613060</td>\n",
              "      <td>74.984215</td>\n",
              "      <td>73.644890</td>\n",
              "      <td>73.561569</td>\n",
              "      <td>72.633400</td>\n",
              "      <td>72.104858</td>\n",
              "      <td>69.013458</td>\n",
              "      <td>61.039642</td>\n",
              "      <td>57.082241</td>\n",
              "      <td>53.847214</td>\n",
              "      <td>49.566376</td>\n",
              "      <td>42.403500</td>\n",
              "      <td>33.994095</td>\n",
              "      <td>27.526628</td>\n",
              "      <td>26.365353</td>\n",
              "      <td>29.282284</td>\n",
              "      <td>33.459450</td>\n",
              "      <td>36.201546</td>\n",
              "      <td>40.116142</td>\n",
              "      <td>50.247913</td>\n",
              "      <td>54.396042</td>\n",
              "      <td>51.849800</td>\n",
              "      <td>47.334759</td>\n",
              "      <td>47.492828</td>\n",
              "      <td>52.386414</td>\n",
              "      <td>53.143311</td>\n",
              "      <td>78.596443</td>\n",
              "      <td>77.068474</td>\n",
              "      <td>77.964386</td>\n",
              "      <td>78.948502</td>\n",
              "      <td>77.553276</td>\n",
              "      <td>76.244553</td>\n",
              "      <td>73.587700</td>\n",
              "      <td>74.381035</td>\n",
              "      <td>75.119446</td>\n",
              "      <td>74.461830</td>\n",
              "      <td>75.530251</td>\n",
              "      <td>...</td>\n",
              "      <td>67.273636</td>\n",
              "      <td>65.459450</td>\n",
              "      <td>60.861961</td>\n",
              "      <td>59.183163</td>\n",
              "      <td>59.207649</td>\n",
              "      <td>58.753742</td>\n",
              "      <td>53.258114</td>\n",
              "      <td>30.709381</td>\n",
              "      <td>15.925364</td>\n",
              "      <td>15.167537</td>\n",
              "      <td>15.832875</td>\n",
              "      <td>15.966616</td>\n",
              "      <td>81.998489</td>\n",
              "      <td>78.338638</td>\n",
              "      <td>64.921227</td>\n",
              "      <td>47.825832</td>\n",
              "      <td>50.417782</td>\n",
              "      <td>54.491486</td>\n",
              "      <td>57.599915</td>\n",
              "      <td>61.921223</td>\n",
              "      <td>64.011337</td>\n",
              "      <td>60.120125</td>\n",
              "      <td>57.503387</td>\n",
              "      <td>59.755135</td>\n",
              "      <td>64.256973</td>\n",
              "      <td>65.708656</td>\n",
              "      <td>65.505096</td>\n",
              "      <td>65.434807</td>\n",
              "      <td>64.853989</td>\n",
              "      <td>63.665646</td>\n",
              "      <td>60.225136</td>\n",
              "      <td>59.134720</td>\n",
              "      <td>59.360435</td>\n",
              "      <td>50.466381</td>\n",
              "      <td>30.458569</td>\n",
              "      <td>17.284302</td>\n",
              "      <td>15.882303</td>\n",
              "      <td>16.541586</td>\n",
              "      <td>16.514517</td>\n",
              "      <td>15.995704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>158.0</td>\n",
              "      <td>75.679695</td>\n",
              "      <td>55.978371</td>\n",
              "      <td>55.136673</td>\n",
              "      <td>55.360359</td>\n",
              "      <td>53.118889</td>\n",
              "      <td>54.736580</td>\n",
              "      <td>57.474442</td>\n",
              "      <td>58.342571</td>\n",
              "      <td>60.001762</td>\n",
              "      <td>59.938633</td>\n",
              "      <td>59.873096</td>\n",
              "      <td>61.153019</td>\n",
              "      <td>61.812210</td>\n",
              "      <td>63.185711</td>\n",
              "      <td>63.533409</td>\n",
              "      <td>62.949211</td>\n",
              "      <td>66.519463</td>\n",
              "      <td>69.450729</td>\n",
              "      <td>67.152855</td>\n",
              "      <td>64.610641</td>\n",
              "      <td>69.205894</td>\n",
              "      <td>66.367249</td>\n",
              "      <td>31.425892</td>\n",
              "      <td>14.368050</td>\n",
              "      <td>15.628105</td>\n",
              "      <td>14.492390</td>\n",
              "      <td>7.855952</td>\n",
              "      <td>0.362923</td>\n",
              "      <td>84.832397</td>\n",
              "      <td>73.780319</td>\n",
              "      <td>62.278801</td>\n",
              "      <td>58.169205</td>\n",
              "      <td>58.889118</td>\n",
              "      <td>58.780159</td>\n",
              "      <td>57.333435</td>\n",
              "      <td>57.797146</td>\n",
              "      <td>56.957695</td>\n",
              "      <td>60.165195</td>\n",
              "      <td>62.278805</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.032046</td>\n",
              "      <td>0.021791</td>\n",
              "      <td>0.043583</td>\n",
              "      <td>0.028201</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>79.731453</td>\n",
              "      <td>36.595417</td>\n",
              "      <td>3.161833</td>\n",
              "      <td>0.396251</td>\n",
              "      <td>0.471078</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.355712</td>\n",
              "      <td>0.620253</td>\n",
              "      <td>0.037975</td>\n",
              "      <td>0.031405</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.257971</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>142.0</td>\n",
              "      <td>54.219994</td>\n",
              "      <td>42.878597</td>\n",
              "      <td>28.404085</td>\n",
              "      <td>15.949416</td>\n",
              "      <td>13.578259</td>\n",
              "      <td>15.041460</td>\n",
              "      <td>15.948225</td>\n",
              "      <td>18.674667</td>\n",
              "      <td>28.957947</td>\n",
              "      <td>36.214443</td>\n",
              "      <td>40.003773</td>\n",
              "      <td>41.034519</td>\n",
              "      <td>43.924221</td>\n",
              "      <td>44.252731</td>\n",
              "      <td>43.426105</td>\n",
              "      <td>43.831978</td>\n",
              "      <td>41.524101</td>\n",
              "      <td>64.597702</td>\n",
              "      <td>87.093445</td>\n",
              "      <td>95.267212</td>\n",
              "      <td>102.017456</td>\n",
              "      <td>109.296173</td>\n",
              "      <td>112.249947</td>\n",
              "      <td>114.267006</td>\n",
              "      <td>120.438019</td>\n",
              "      <td>135.351318</td>\n",
              "      <td>139.618927</td>\n",
              "      <td>125.905777</td>\n",
              "      <td>26.261656</td>\n",
              "      <td>14.974211</td>\n",
              "      <td>9.968855</td>\n",
              "      <td>10.644119</td>\n",
              "      <td>12.283673</td>\n",
              "      <td>13.688157</td>\n",
              "      <td>14.577068</td>\n",
              "      <td>18.079550</td>\n",
              "      <td>26.241417</td>\n",
              "      <td>34.937313</td>\n",
              "      <td>39.194012</td>\n",
              "      <td>...</td>\n",
              "      <td>123.296768</td>\n",
              "      <td>123.647095</td>\n",
              "      <td>121.030754</td>\n",
              "      <td>118.947029</td>\n",
              "      <td>117.874443</td>\n",
              "      <td>117.627655</td>\n",
              "      <td>116.539772</td>\n",
              "      <td>111.166245</td>\n",
              "      <td>96.440781</td>\n",
              "      <td>55.556442</td>\n",
              "      <td>21.793493</td>\n",
              "      <td>39.841106</td>\n",
              "      <td>84.672287</td>\n",
              "      <td>84.525887</td>\n",
              "      <td>85.090454</td>\n",
              "      <td>88.696884</td>\n",
              "      <td>89.052383</td>\n",
              "      <td>91.327324</td>\n",
              "      <td>94.964897</td>\n",
              "      <td>98.433640</td>\n",
              "      <td>99.285255</td>\n",
              "      <td>99.290421</td>\n",
              "      <td>97.652649</td>\n",
              "      <td>105.313431</td>\n",
              "      <td>114.339615</td>\n",
              "      <td>125.652054</td>\n",
              "      <td>131.249969</td>\n",
              "      <td>130.963898</td>\n",
              "      <td>125.453087</td>\n",
              "      <td>127.512604</td>\n",
              "      <td>128.810165</td>\n",
              "      <td>127.942871</td>\n",
              "      <td>126.204536</td>\n",
              "      <td>124.499313</td>\n",
              "      <td>120.913116</td>\n",
              "      <td>111.557037</td>\n",
              "      <td>88.758186</td>\n",
              "      <td>38.976395</td>\n",
              "      <td>23.533625</td>\n",
              "      <td>29.693117</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width          0          1  ...         781        782        783\n",
              "3  192.0  74.573349  77.677505  ...  100.246094  55.891491  49.351994\n",
              "4  137.0  40.407211  44.869568  ...   87.582230  86.839935  86.061630\n",
              "5  139.0  88.045952  81.073753  ...   16.541586  16.514517  15.995704\n",
              "7  158.0  75.679695  55.978371  ...    0.000000   0.000000   0.000000\n",
              "8  142.0  54.219994  42.878597  ...   38.976395  23.533625  29.693117\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "Area = np.array(PSD_new['Area'])\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J705kDqsE8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c3043b-cbaa-4678-c1b5-abb18cdc6253"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfk_fNXGDK5_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "8b288602-8dd1-4934-ec2b-c7a9d5541c4d"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f45229a0e50>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nGDbBEeiUij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "753c930f-7a65-474b-ff08-c14f0cc2fd8b"
      },
      "source": [
        "# plt.hist(x, bins=bins, density=True, histtype='step', cumulative=-1,label='Reversed emp.')\n",
        "plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.        , 0.02020202, 0.08080808, 0.26262626, 0.51515152,\n",
              "         0.68686869, 0.81818182, 0.91919192, 0.96969697, 1.        ],\n",
              "        [0.03030303, 0.09090909, 0.21212121, 0.39393939, 0.63636364,\n",
              "         0.78787879, 0.84848485, 0.93939394, 1.        , 1.        ]]),\n",
              " array([0.73684937, 0.85455697, 0.97226456, 1.08997216, 1.20767975,\n",
              "        1.32538735, 1.44309495, 1.56080254, 1.67851014, 1.79621773,\n",
              "        1.91392533]),\n",
              " <a list of 2 Lists of Patches objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xENlBUUxfTu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "6d1394d5-602c-4640-867c-0d08c6151221"
      },
      "source": [
        "Obj = plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')\n",
        "Y1, Y2 = Obj[0]\n",
        "Rsquared = r2_score(Y1, Y2)\n",
        "print('r_squared =',Rsquared)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r_squared = 0.9545940903728497\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KukfpGTTKlj"
      },
      "source": [
        "#df"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bT8GFymJAII"
      },
      "source": [
        "# r.history['accuracy']"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uPdRxL2VwLR"
      },
      "source": [
        "acc_train = r.history['accuracy'][-1]\n",
        "acc_test = r.history['val_accuracy'][-1]\n",
        "loss_train = r.history['loss'][-1]\n",
        "loss_test = r.history['val_loss'][-1]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTd_-CYN1v0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "07515979-9971-4238-9ca6-d222351f1954"
      },
      "source": [
        "\n",
        "df = pd.DataFrame({'N1':N1, 'N2':N2,'R^2':Rsquared,\n",
        "                   'acc train':acc_train,'acc test':acc_test,\n",
        "                   'loss train':loss_train,'loss test':loss_test,\n",
        "                   'Details':Description},\n",
        "                  index= [0])\n",
        "Arq = \"output.xlsx\"\n",
        "df.to_excel(Arq)\n",
        "files.download(Arq)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d411ab50-5cca-4445-836a-96647924c480\", \"output.xlsx\", 5146)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZHa1j4HT9Dq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "475df390-ba1d-451e-d6a5-a53f3b42aecc"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<a list of 2 Lists of Patches objects>\n",
            "[0.73684937 0.85455697 0.97226456 1.08997216 1.20767975 1.32538735\n",
            " 1.44309495 1.56080254 1.67851014 1.79621773 1.91392533]\n",
            "[[ 0.          2.02020202  6.06060606 18.18181818 25.25252525 17.17171717\n",
            "  13.13131313 10.1010101   5.05050505  3.03030303]\n",
            " [ 3.03030303  6.06060606 12.12121212 18.18181818 24.24242424 15.15151515\n",
            "   6.06060606  9.09090909  6.06060606  0.        ]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o_vDGeWUwIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce1cd41-4513-40c5-ebcf-88b6760f00d3"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200.00000000000003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcH52-6iJQ8t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "c52b4aa5-1894-4101-e5d8-53c01da9822d"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f452634fa50>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAR8klEQVR4nO3dfbDWZZ3H8fdXRM/sSkpyJBTxoJEKY4AeoVamCNRQZzRnzKRd0kYH02Ri6w9JZ1fabUYsSrMsF8vRJawck03HttVxcR1LU1BU7EzmA+JxkScde9jUkO/+cW4J8cC5zzn3w7kO79fMmfN7vH/f6wAfrnPd1+93R2YiSSrPXs0uQJLUNwa4JBXKAJekQhngklQoA1ySCrV3Iy82YsSIbGtra+QlJal4q1at2pyZrTtvb2iAt7W1sXLlykZeUpKKFxEvdLfdIRRJKpQBLkmFMsAlqVANHQOXtGf7y1/+QmdnJ6+//nqzSxmQWlpaGD16NEOHDq3qeANcUsN0dnYybNgw2traiIhmlzOgZCZbtmyhs7OTsWPHVnWOQyiSGub111/nwAMPNLy7EREceOCBvfrtpMcAj4hDI2JFRPwmIp6KiC9Uti+MiJciYnXl69R+1C5pD2F471pvfzbVDKFsBb6UmY9GxDBgVUTcU9l3dWYu7mWNkqQa6DHAM3M9sL6y/IeI6AAOqXdhkga/tgV31fT11i46rcdj9ttvP/74xz/W9Lp9MX36dBYvXkx7e3ufX6NXb2JGRBswGfg1cAJwSUR8BlhJVy/91W7OmQvMBRgzZkyfC9WepS//sKv5xysNJlW/iRkR+wE/BeZn5u+B7wFHAJPo6qF/o7vzMnNJZrZnZntr67tu5Zekprjvvvv46Ec/yhlnnMHhhx/OggULWLZsGVOmTOGYY47h2WefBeDOO+9k6tSpTJ48mRNPPJENGzYAsGnTJk466SQmTJjABRdcwGGHHcbmzZsB+OEPf8iUKVOYNGkSF154IW+99VZd2lBVgEfEULrCe1lm3g6QmRsy863M3AbcAEypS4WSVCePP/44119/PR0dHSxdupSnn36ahx9+mAsuuIBvf/vbAEybNo2HHnqIxx57jHPOOYevfe1rAHzlK19hxowZPPXUU5x11lmsW7cOgI6ODn7yk5/wy1/+ktWrVzNkyBCWLVtWl/p7HEKJrrdFfwB0ZOY3d9g+qjI+DnAmsKYuFUpSnRx//PGMGjUKgCOOOIKTTz4ZgGOOOYYVK1YAXXPXP/WpT7F+/XrefPPN7XO0H3jgAZYvXw7ArFmzGD58OAD33nsvq1at4vjjjwfgz3/+MwcddFBd6q9mDPwEYA7wZESsrmy7DJgdEZOABNYCF9alQkmqk3333Xf78l577bV9fa+99mLr1q0AzJs3jy9+8Yucfvrp3HfffSxcuHC3r5mZnHvuuVx55ZV1q/ttPQ6hZOYDmRmZ+cHMnFT5+nlmzsnMYyrbT9+hNy5Jg8Zrr73GIYd0Tby7+eabt28/4YQTuPXWWwG4++67efXVrjkcM2fO5LbbbmPjxo0AvPLKK7zwQrdPg+03b6WX1DQlzBxauHAhn/zkJxk+fDgzZszg+eefB+CKK65g9uzZLF26lA9/+MO8733vY9iwYYwYMYKvfvWrnHzyyWzbto2hQ4dy3XXXcdhhh73jdbdu3fqO3wD6IjKzXy/QG+3t7ekHOqgaTiMcnDo6Ojj66KObXUZNvPHGGwwZMoS9996bBx98kIsuuojVq1f3fGLl3Pe///2sWbOG/fff/x37uvsZRcSqzHzXhHF74JLUB+vWrePss89m27Zt7LPPPtxwww1Vnbdy5UrmzJnDxRdf/K7w7i0DXJL6YNy4cTz22GO9Pq+9vZ2Ojo6a1ODTCCWpUAa4JBXKAJekQhngklQo38SU1DwL+zcL492v91qPh7z88svMnz+fRx55hAMOOICRI0dyzTXXcOSRR3Lttdcyb948AC655BLa29s577zzOO+887jnnnt47rnn2Hfffdm8eTPt7e2sXbu2tvX3kj1wSXuMzOTMM89k+vTpPPvss6xatYorr7ySDRs2cNBBB/Gtb32LN998s9tzhwwZwo033tjginfPAJe0x1ixYgVDhw7lc5/73PZtEydO5NBDD6W1tZWZM2e+43b5Hc2fP5+rr756+zNSBgIDXNIeY82aNRx33HG73H/ppZeyePHibp/fPWbMGKZNm8bSpUvrWWKvGOCSVHH44YczdepUbrnllm73f/nLX+brX/8627Zta3Bl3TPAJe0xJkyYwKpVq3Z7zGWXXcZVV11Fd8+JGjduHJMmTdr+FMJmM8Al7TFmzJjBG2+8wZIlS7Zve+KJJ3jxxRe3rx911FGMHz+eO++8s9vXuPzyy1m8eHHda62G0wglNU8V0/5qKSJYvnw58+fP56qrrqKlpYW2tjauueaadxx3+eWXM3ny5G5fY8KECRx77LE8+uijjSh5twxwSXuUgw8+uNshkDVr/vqpkBMnTnzHOPdNN930jmNvv/32utXXGw6hSFKhDHBJKpQBLqmhGvkpYKXp7c/GAJfUMC0tLWzZssUQ70ZmsmXLFlpaWqo+xzcxJTXM6NGj6ezsZNOmTc0uZUBqaWlh9OjRVR9vgEtqmKFDhzJ27NhmlzFoOIQiSYUywCWpUAa4JBXKAJekQhngklQoA1ySCmWAS1KhDHBJKpQBLkmF6jHAI+LQiFgREb+JiKci4guV7e+NiHsi4neV78PrX64k6W3V9MC3Al/KzPHAh4DPR8R4YAFwb2aOA+6trEuSGqTHAM/M9Zn5aGX5D0AHcAhwBnBz5bCbgU/Uq0hJ0rv1agw8ItqAycCvgZGZub6y62Vg5C7OmRsRKyNipU8gk6TaqTrAI2I/4KfA/Mz8/Y77suvhvt0+4Dczl2Rme2a2t7a29qtYSdJfVRXgETGUrvBelplvf5rnhogYVdk/CthYnxIlSd2pZhZKAD8AOjLzmzvsugM4t7J8LvCz2pcnSdqVaj7Q4QRgDvBkRKyubLsMWATcGhHnAy8AZ9enRElSd3oM8Mx8AIhd7J5Z23IkSdXyTkxJKpQBLkmFMsAlqVAGuCQVygCXpEIZ4JJUKANckgplgEtSoQxwSSqUAS5JhTLAJalQ1TzMSnuotgV39fqctYtOq0MlkrpjD1ySCmWAS1KhDHBJKpQBLkmFMsAlqVAGuCQVygCXpEI5D1zaifPfVQp74JJUKANckgplgEtSoQxwSSqUAS5JhTLAJalQBrgkFcoAl6RCGeCSVCgDXJIKZYBLUqEMcEkqVI8BHhE3RsTGiFizw7aFEfFSRKyufJ1a3zIlSTurpgd+EzCrm+1XZ+akytfPa1uWJKknPQZ4Zt4PvNKAWiRJvdCfMfBLIuKJyhDL8JpVJEmqSl8D/HvAEcAkYD3wjV0dGBFzI2JlRKzctGlTHy8nSdpZnwI8Mzdk5luZuQ24AZiym2OXZGZ7Zra3trb2tU5J0k76FOARMWqH1TOBNbs6VpJUHz1+JmZE/AiYDoyIiE7gCmB6REwCElgLXFjHGiVJ3egxwDNzdjebf1CHWiRJveCdmJJUKANckgplgEtSoQxwSSqUAS5JhTLAJalQBrgkFcoAl6RCGeCSVCgDXJIKZYBLUqF6fBaKpMZqW3BXr45fu+i0OlWigc4euCQVygCXpEIZ4JJUKANckgplgEtSoQxwSSqUAS5JhTLAJalQBrgkFcoAl6RCGeCSVCgDXJIKZYBLUqEMcEkqlAEuSYUywCWpUAa4JBXKAJekQhngklQoA1ySCmWAS1KhegzwiLgxIjZGxJodtr03Iu6JiN9Vvg+vb5mSpJ1V0wO/CZi107YFwL2ZOQ64t7IuSWqgHgM8M+8HXtlp8xnAzZXlm4FP1LguSVIP9u7jeSMzc31l+WVg5K4OjIi5wFyAMWPG9PFykhqhbcFdvTp+7aLT6lSJqtHvNzEzM4Hczf4lmdmeme2tra39vZwkqaKvAb4hIkYBVL5vrF1JkqRq9DXA7wDOrSyfC/ysNuVIkqpVzTTCHwEPAkdGRGdEnA8sAk6KiN8BJ1bWJUkN1OObmJk5exe7Zta4FklSL3gnpiQVygCXpEIZ4JJUKANckgplgEtSoQxwSSqUAS5JhTLAJalQBrgkFcoAl6RCGeCSVKi+fqCDNPAs3L+Xx79WnzqkBrEHLkmFMsAlqVAGuCQVygCXpEIZ4JJUKANckgplgEtSoQxwSSqUAS5JhTLAJalQBrgkFcoAl6RCGeCSVCgDXJIKZYBLUqF8HvgA17bgrl6fs3bRaXWopEo+k1tqGHvgklQoA1ySCmWAS1KhDHBJKlS/3sSMiLXAH4C3gK2Z2V6LoiRJPavFLJSPZebmGryOJKkXHEKRpEL1tweewN0RkcC/ZeaSnQ+IiLnAXIAxY8b083LSAOX8937r7T0PTb3fYYDobw98WmYeC5wCfD4iPrLzAZm5JDPbM7O9tbW1n5eTJL2tXwGemS9Vvm8ElgNTalGUJKlnfQ7wiPjbiBj29jJwMrCmVoVJknavP2PgI4HlEfH269ySmb+oSVWSpB71OcAz8zlgYg1rkST1gtMIJalQBrgkFcoAl6RC+YEOg5E3lUh7BHvgklQoA1ySCmWAS1KhDHBJKpQBLkmFMsAlqVAGuCQVynngkvpukNxzUOqHSdgDl6RCGeCSVCgDXJIKZYBLUqEMcEkqlAEuSYUywCWpUM4Dr0Jv54gCrG35dO9OGKDzY1WAQTIXu9f21HbvwB64JBXKAJekQhngklQoA1ySCmWAS1KhDHBJKpQBLkmFMsAlqVCD+0YeJ/pLGsTsgUtSoQxwSSqUAS5JhTLAJalQ/QrwiJgVEb+NiGciYkGtipIk9azPAR4RQ4DrgFOA8cDsiBhfq8IkSbvXnx74FOCZzHwuM98EfgycUZuyJEk9iczs24kRZwGzMvOCyvocYGpmXrLTcXOBuZXVI4Hf9r3cPhkBbG7wNetpsLUHbFMpbFPzHJaZrTtvrPuNPJm5BFhS7+vsSkSszMz2Zl2/1gZbe8A2lcI2DTz9GUJ5CTh0h/XRlW2SpAboT4A/AoyLiLERsQ9wDnBHbcqSJPWkz0Mombk1Ii4B/gsYAtyYmU/VrLLaadrwTZ0MtvaAbSqFbRpg+vwmpiSpubwTU5IKZYBLUqEGRYD3dEt/RIyJiBUR8VhEPBERpzajzt6IiBsjYmNErNnF/oiIayttfiIijm10jb1RRXv+vtKOJyPiVxExsdE19lZPbdrhuOMjYmvl3okBrZo2RcT0iFgdEU9FxP80sr6+qOLv3v4RcWdEPF5p02cbXWOfZWbRX3S9gfoscDiwD/A4MH6nY5YAF1WWxwNrm113Fe36CHAssGYX+08F/hMI4EPAr5tdcz/b83fA8MryKQO9PdW0qXLMEOC/gZ8DZzW75hr8OR0A/AYYU1k/qNk116BNlwFXVZZbgVeAfZpddzVfg6EHXs0t/Qm8p7K8P/C/DayvTzLzfrr+Iu3KGcC/Z5eHgAMiYlRjquu9ntqTmb/KzFcrqw/RdV/BgFbFnxHAPOCnwMb6V9R/VbTp08DtmbmucvyAb1cVbUpgWEQEsF/l2K2NqK2/BkOAHwK8uMN6Z2XbjhYC/xARnXT1hOY1prS6qqbdpTqfrt8uihYRhwBnAt9rdi019AFgeETcFxGrIuIzzS6oBr4DHE1Xx+5J4AuZua25JVVnMAR4NWYDN2XmaLqGHpZGxJ7S9qJExMfoCvBLm11LDVwDXFpKGFRpb+A44DTg48A/RcQHmltSv30cWA0cDEwCvhMR79n9KQPDYPhQ42pu6T8fmAWQmQ9GRAtdD7EZ8L/+7cage5RBRHwQ+D5wSmZuaXY9NdAO/LjrN3NGAKdGxNbM/I/mltUvncCWzPwT8KeIuB+YCDzd3LL65bPAouwaBH8mIp4HjgIebm5ZPRsMvdBqbulfB8wEiIijgRZgU0OrrL07gM9UZqN8CHgtM9c3u6i+iogxwO3AnMwsOQy2y8yxmdmWmW3AbcDFhYc3wM+AaRGxd0T8DTAV6GhyTf21Yz6MpOupqc81taIqFd8Dz13c0h8R/wKszMw7gC8BN0TEP9L1hsV5lf9tB6yI+BEwHRhRGbu/AhgKkJnX0zWWfyrwDPB/dPUiBqwq2vPPwIHAdys91q05wJ8SV0WbitNTmzKzIyJ+ATwBbAO+n5m7nUbZbFX8Of0rcFNEPEnXrK5LM7OER8x6K70klWowDKFI0h7JAJekQhngklQoA1ySCmWAS1KhDHBJKpQBLkmF+n8Wz57dQurzRwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r11AxFK_JIii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8680acb3-c4fc-48b6-a339-501bc2b6397a"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.0136588738827657,\n",
              "  1.3384498643738487,\n",
              "  1.2241743928678164,\n",
              "  1.2810955363139882,\n",
              "  1.4897169283032898,\n",
              "  1.3469841688927182,\n",
              "  1.802936648279376,\n",
              "  1.1101783096888704,\n",
              "  1.349345229721172,\n",
              "  0.9587903583676608,\n",
              "  1.2539740179716348,\n",
              "  1.1606409601292735,\n",
              "  1.3071677279655385,\n",
              "  1.2396774300177211,\n",
              "  1.4634168627716928,\n",
              "  1.369947111730825,\n",
              "  1.5945718456175375,\n",
              "  1.2293637946517857,\n",
              "  1.6281537802488464,\n",
              "  1.9139253303624628,\n",
              "  1.3284238815238665,\n",
              "  1.2236542422631285,\n",
              "  1.1055812783082735,\n",
              "  1.2084724894722394,\n",
              "  1.1396070970426018,\n",
              "  1.2610615335399848,\n",
              "  1.4729562736988213,\n",
              "  1.428191076384995,\n",
              "  1.2880338220675407,\n",
              "  1.474252319944901,\n",
              "  1.697826354277848,\n",
              "  1.2529582429827641,\n",
              "  1.034176589165282,\n",
              "  1.1861406733319428,\n",
              "  1.3076546595257188,\n",
              "  1.016167934339702,\n",
              "  1.607692842495848,\n",
              "  1.5322707725763225,\n",
              "  1.6985761120332168,\n",
              "  1.3690173884396646,\n",
              "  1.1334454996327699,\n",
              "  1.5351763407845973,\n",
              "  1.09458595441189,\n",
              "  1.6933208363283037,\n",
              "  1.2711180048559307,\n",
              "  1.1737313097142148,\n",
              "  1.1644741796806608,\n",
              "  1.59377316194829,\n",
              "  1.6207076173044417,\n",
              "  1.110751600875899,\n",
              "  1.329860792578359,\n",
              "  1.480285532081299,\n",
              "  1.3134837582232015,\n",
              "  1.224694322554825,\n",
              "  1.3952717527062835,\n",
              "  1.3212158853994709,\n",
              "  1.3460385849440337,\n",
              "  1.8732453617425853,\n",
              "  1.0427591146587158,\n",
              "  1.016167934339702,\n",
              "  0.9373021315815206,\n",
              "  1.4935580612671222,\n",
              "  1.1158979678944616,\n",
              "  1.1877497276642752,\n",
              "  1.2095256247391792,\n",
              "  1.1249889365974903,\n",
              "  1.4246205931081612,\n",
              "  1.3782864001160509,\n",
              "  1.2973907191512837,\n",
              "  1.411150823960995,\n",
              "  1.2815923738491737,\n",
              "  1.1600923233885598,\n",
              "  1.2324669475767085,\n",
              "  1.341300666036808,\n",
              "  1.3713405148152793,\n",
              "  1.6774562271083886,\n",
              "  1.2716187407449044,\n",
              "  1.540971041561482,\n",
              "  1.1769811488175403,\n",
              "  1.7172136691765054,\n",
              "  1.5368341990871126,\n",
              "  1.0597156592484673,\n",
              "  1.4214890634453377,\n",
              "  1.169928421139644,\n",
              "  1.6656497134685673,\n",
              "  1.4922787821790537,\n",
              "  1.103852455861539,\n",
              "  1.2565098628628435,\n",
              "  1.4210411387253952,\n",
              "  1.7866201439172942,\n",
              "  1.540557857201846,\n",
              "  1.2771138777750963,\n",
              "  1.6495189926457479,\n",
              "  1.2776122636975893,\n",
              "  1.6340083614564633,\n",
              "  1.5434478249035466,\n",
              "  1.2939513133525307,\n",
              "  1.1474012764748687,\n",
              "  1.6332289631953525],\n",
              " [1.6753315010349805,\n",
              "  1.0116044473468049,\n",
              "  1.155686722943429,\n",
              "  1.2452701435617113,\n",
              "  1.22630607160556,\n",
              "  1.3306753068879702,\n",
              "  0.9787930429893044,\n",
              "  0.8670049221540209,\n",
              "  1.6729872274153617,\n",
              "  1.387010576873647,\n",
              "  1.2138918267911087,\n",
              "  1.2611910157283146,\n",
              "  1.2832585811219976,\n",
              "  1.37137387521741,\n",
              "  1.2979901906268694,\n",
              "  1.2382904698513322,\n",
              "  0.736849368964125,\n",
              "  1.0514441277580238,\n",
              "  1.0012877121225658,\n",
              "  1.4272768110517857,\n",
              "  1.430246219574811,\n",
              "  1.5184181975399123,\n",
              "  1.606485337401383,\n",
              "  1.1095816743720852,\n",
              "  1.5340448893432397,\n",
              "  1.7914135902854844,\n",
              "  0.8896087725824072,\n",
              "  1.135788672302879,\n",
              "  1.3134296968531562,\n",
              "  1.1095421874516418,\n",
              "  1.1065068779945106,\n",
              "  1.709557956295675,\n",
              "  1.1734551359140732]]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}